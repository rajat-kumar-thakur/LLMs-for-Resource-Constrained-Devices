{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b725ce54-969e-49bf-b560-d6d33780dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate peft datasets pandas scikit-learn numpy matplotlib\n",
    "!pip install -q bitsandbytes safetensors\n",
    "\n",
    "!pip install -q awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0354d5-7111-4d0f-969a-3f98c7eedf6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be5d2b4c41e4615a11eb9c68fe468e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size: 85000\n",
      "Sample prompts: [\"Generate Python code for: Pelvic Girdle Pain during or after Pregnancy: a review of recent evidence and a clinical care path proposal\\nAbstract: PROBLEM STATEMENT\\nPelvic girdle pain (PGP) is a common condition during or after pregnancy with pain and disability as most important symptoms. These symptoms have a wide range of clinical presentation. Most doctors perceive pregnancy related pelvic girdle pain (PPGP) as 'physiologic' or 'expected during pregnancy', where no treatment is needed. As such women with PPGP mostly experience little rec\", 'Generate Python code for: Packet Classification Using Tuple Space Search\\nAbstract: Routers must perform packet classification at high speeds to efficiently implement functions such as firewalls and QoS routing. Packet classification requires matching each packet against a database of filters (or rules), and forwarding the packet according to the highest priority filter. Existing filter schemes with fast lookup time do not scale to large filter databases. Other more scalable sche', 'Generate Python code for: Bayesian Compressive Sensing\\nAbstract: The data of interest are assumed to be represented as N-dimensional real vectors, and these vectors are compressible in some linear basis B, implying that the signal can be reconstructed accurately using only a small number M Lt N of basis-function coefficients associated with B. Compressive sensing is a framework whereby one does not measure one of the aforementioned N-dimensional signals directl']\n",
      "Sample codes: ['import numpy as np\\nimport scipy\\nfrom scipy import stats\\n\\nfrom networkx.algorithms import bipartite\\ni...', '#!/usr/bin/env python\\n# -*- coding: utf-8 -*-\\n# @Time    : 5/15/20 4:49 PM\\n# @File    : grover.py\\n\\n#...', '# This file is part of the pyMOR project (http://www.pymor.org).\\n# Copyright 2013-2020 pyMOR develop...']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from datasets import Dataset, load_dataset\n",
    "from typing import List\n",
    "\n",
    "def prepare_datasets() -> Dataset:\n",
    "    \"\"\"Prepare dataset using only real data sources\"\"\"\n",
    "    all_prompts: List[str] = []\n",
    "    all_codes: List[str] = []\n",
    "    \n",
    "    def load_scidocs() -> List[str]:\n",
    "        try:\n",
    "            os.makedirs(\"scidocs_data\", exist_ok=True)\n",
    "            if not os.path.exists(\"scidocs_data/paper_metadata_view_cite_read.json\"):\n",
    "                subprocess.run([\n",
    "                    \"aws\", \"s3\", \"sync\", \"--no-sign-request\",\n",
    "                    \"s3://ai2-s2-research-public/specter/scidocs/\",\n",
    "                    \"scidocs_data/\", \"--region\", \"us-west-2\", \"--quiet\"\n",
    "                ], check=True)\n",
    "            \n",
    "            with open(\"scidocs_data/paper_metadata_view_cite_read.json\", \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            prompts = []\n",
    "            for paper_id, content in data.items():\n",
    "                title = content.get('title', '') or ''\n",
    "                abstract = content.get('abstract', '') or ''\n",
    "                \n",
    "                if len(title) > 10 and len(abstract) > 200:\n",
    "                    prompts.append(\n",
    "                        f\"Generate Python code for: {title}\\nAbstract: {abstract[:400]}\"\n",
    "                    )\n",
    "            return prompts\n",
    "        except Exception as e:\n",
    "            print(f\"SciDocs loading failed: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def load_astronomy() -> List[str]:\n",
    "        try:\n",
    "            ds = load_dataset(\"David-Xu/astronomy-stack-dpo-text\", split=\"train\")\n",
    "            return [example['prompt'] for example in ds]\n",
    "        except Exception as e:\n",
    "            print(f\"Astronomy dataset loading failed: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def load_science() -> List[str]:\n",
    "        try:\n",
    "            ds = load_dataset(\"millawell/wikipedia_field_of_science\", split=\"train\")\n",
    "            return [text for text in ds['text'] if len(text) > 30]\n",
    "        except Exception as e:\n",
    "            print(f\"Science dataset loading failed: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def load_code_samples() -> List[str]:\n",
    "        try:\n",
    "            ds = load_dataset(\"bigcode/the-stack\", \n",
    "                            data_dir=\"data/python\", \n",
    "                            split=\"train\",\n",
    "                            streaming=True)\n",
    "            \n",
    "            samples = []\n",
    "            for sample in ds:\n",
    "                content = sample[\"content\"]\n",
    "                if any(imp in content for imp in [\"numpy\", \"sklearn\", \"pandas\", \"matplotlib\"]):\n",
    "                    if \"auto-generated\" not in content.lower():\n",
    "                        samples.append(content[:2000])\n",
    "                        if len(samples) >= 20000:\n",
    "                            break\n",
    "            return samples\n",
    "        except Exception as e:\n",
    "            print(f\"Code dataset loading failed: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    # Load all datasets\n",
    "    scidocs = load_scidocs()[:25000]  # Cap at 25k\n",
    "    astronomy = load_astronomy()[:15000]  # Cap at 15k\n",
    "    science = load_science()[:15000]  # Cap at 15k\n",
    "    code_samples = load_code_samples()[:20000]  # Cap at 20k\n",
    "    \n",
    "    science_code_prompts = [\n",
    "        f\"Generate Python code for: {text.split(':')[-1].strip()}\" \n",
    "        for text in science[:10000]\n",
    "    ]\n",
    "\n",
    "    # Combine all sources\n",
    "    all_prompts.extend(scidocs)\n",
    "    all_prompts.extend(astronomy)\n",
    "    all_prompts.extend(science)\n",
    "    all_prompts.extend(science_code_prompts)\n",
    "    all_prompts.extend([\"Generate Python code:\"] * len(code_samples))\n",
    "    \n",
    "    all_codes.extend([\"\"] * (len(scidocs) + len(astronomy) + len(science) + len(science_code_prompts)))\n",
    "    all_codes.extend(code_samples)\n",
    "\n",
    "    # Final dataset\n",
    "    return Dataset.from_dict({\n",
    "        \"prompt\": all_prompts,\n",
    "        \"code\": all_codes\n",
    "    })\n",
    "\n",
    "dataset = prepare_datasets()\n",
    "print(f\"Final dataset size: {len(dataset)}\")\n",
    "print(\"Sample prompts:\", dataset[\"prompt\"][:3])\n",
    "print(\"Sample codes:\", [c[:100] + \"...\" if c else \"\" for c in dataset[\"code\"][-3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169b67a-6c6f-4233-9e2e-32618531bac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 64885\n",
      "Prompt: Generate Python code for: Sachs Patera\n",
      "  \n",
      "  Sachs Patera is a feature on Venus. Defined as a sag-caldera, Sachs is an elliptical depression 130 meters (81Â feet) in depth, spanning in width along its longest axis. The morphology implies that a chamber of molten material drained and collapsed, forming a depression surrounded by concentric scarps spaced apart. The arc-shaped set of scarps, extending out to the north from the prominent ellipse, is evidence for a separate episode of withdrawal; the small lobe-shaped extension to the southwest may represent an additional event. Solidified lava flows long give the caldera its flower-like appearance. The flows are a lighter tone of gray in the radar data because the lava is blockier in texture and consequently returns more radar waves. Much of the lava, which was evacuated from the chamber, probably traveled to other locations underground, while some of it may have surfaced further south. This is unlike calderas on Earth, where a rim of lava builds up in the immediate vicinity of the caldera.\n",
      "Code: \n",
      "\n",
      "Sample 15708\n",
      "Prompt: Generate Python code for: Decentralized learning for wireless communications and networking\n",
      "Abstract: This chapter deals with decentralized learning algorithms for in-network processing of graph-valued data. A generic learning problem is formulated and recast into a separable form, which is iteratively minimized using the alternatingdirection method of multipliers (ADMM) so as to gain the desired degree of parallelization. Without exchanging elements from the distributed training sets and keeping \n",
      "Code: \n",
      "\n",
      "Sample 82508\n",
      "Prompt: Generate Python code:\n",
      "Code: # coding=utf-8\n",
      "# Copyright 2018-2020 EVA\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "import unittest\n",
      "import pandas as pd\n",
      "from mock import Mock\n",
      "\n",
      "from eva.expression.abstract_expression import ExpressionType\n",
      "from eva.expression.comparison_expression import ComparisonExpression\n",
      "from eva.expression.logical_expression import LogicalExpression\n",
      "from eva.expression.constant_value_expression import ConstantValueExpression\n",
      "from eva.expression.tuple_value_expression import TupleValueExpression\n",
      "from eva.models.storage.batch import Batch\n",
      "\n",
      "\n",
      "class LogicalExpressionsTest(unittest.TestCase):\n",
      "\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "\n",
      "    def test_logical_and(self):\n",
      "        const_exp1 = ConstantValueExpression(1)\n",
      "        const_exp2 = ConstantValueExpression(1)\n",
      "\n",
      "        comparison_expression_left = ComparisonExpression(\n",
      "            ExpressionType.COMPARE_EQUAL,\n",
      "            const_exp1,\n",
      "            const_exp2\n",
      "        )\n",
      "        const_exp1 = ConstantValueExpression(2)\n",
      "        const_exp2 = ConstantValueExpression(1)\n",
      "        comparison_expression_right = ComparisonExpression(\n",
      "            ExpressionType.COMPARE_GREATER,\n",
      "            const_exp1,\n",
      "            const_exp2\n",
      "        )\n",
      "        logical_expr = LogicalExpression(\n",
      "            ExpressionType.LOGICAL_AND,\n",
      "            comparison_expression_left,\n",
      "            comparison_expression_right\n",
      "        )\n",
      "        self.assertEqual(\n",
      "            [True], logical_expr.evaluate(None).frames[0].tolist())\n",
      "\n",
      "    def test_logical\n",
      "\n",
      "Sample 70756\n",
      "Prompt: Generate Python code:\n",
      "Code: \n",
      "\"\"\"\n",
      "PERIODS\n",
      "\"\"\"\n",
      "\n",
      "numPeriods = 60\n",
      "\n",
      "\"\"\"\n",
      "STOPS\n",
      "\"\"\"\n",
      "\n",
      "numStations = 6\n",
      "\n",
      "station_names = (\n",
      "\t\"Hamburg Hbf\", #  0\n",
      "\t\"Landwehr\", #  1\n",
      "\t\"Hasselbrook\", #  2\n",
      "\t\"Wansbeker Chaussee*\", #  3\n",
      "\t\"Friedrichsberg*\", #  4\n",
      "\t\"Barmbek*\", #  5\n",
      "\t)\n",
      "\n",
      "numStops = 12\n",
      "\n",
      "stops_position = (\n",
      "\t(0, 0), # Stop 0\n",
      "\t(2, 0), # Stop 1\n",
      "\t(3, 0), # Stop 2\n",
      "\t(4, 0), # Stop 3\n",
      "\t(5, 0), # Stop 4\n",
      "\t(7, 0), # Stop 5\n",
      "\t(7, 1), # Stop 6\n",
      "\t(15, 1), # Stop 7\n",
      "\t(13, 1), # Stop 8\n",
      "\t(12, 1), # Stop 9\n",
      "\t(11, 1), # Stop 10\n",
      "\t(10, 1), # Stop 11\n",
      "\t)\n",
      "\n",
      "stops_distance = (\n",
      "\t(0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), # Stop 0\n",
      "\t(0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), # Stop 1\n",
      "\t(0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), # Stop 2\n",
      "\t(0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), # Stop 3\n",
      "\t(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0), # Stop 4\n",
      "\t(0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), # Stop 5\n",
      "\t(0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0), # Stop 6\n",
      "\t(0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0), # Stop 7\n",
      "\t(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0), # Stop 8\n",
      "\t(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0), # Stop 9\n",
      "\t(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2), # Stop 10\n",
      "\t(1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), # Stop 11\n",
      "\t)\n",
      "\n",
      "station_start = 0\n",
      "\n",
      "\"\"\"\n",
      "TRAMS\n",
      "\"\"\"\n",
      "\n",
      "numTrams = 6\n",
      "\n",
      "tram_capacity = 514\n",
      "\n",
      "tram_capacity_cargo = 304\n",
      "\n",
      "tram_capacity_min_passenger = 208\n",
      "\n",
      "tram_capacity_min_cargo = 0\n",
      "\n",
      "tram_speed = 1\n",
      "\n",
      "tram_headway = 1\n",
      "\n",
      "tram_min_service = 1\n",
      "\n",
      "tram_max_service = 10\n",
      "\n",
      "min_time_next_tram = 0.333\n",
      "\n",
      "tram_travel_deviation = 0.167\n",
      "\n",
      "\"\"\"\n",
      "PASSENGERS\n",
      "\"\"\"\n",
      "\n",
      "passenger_set = \"pas-20210421-2109-int14000000000000001e\"\n",
      "\n",
      "passenger_service_time_board = 0.0145\n",
      "\n",
      "passenger_service_time_alight = 0.0145\n",
      "\n",
      "\"\"\"\n",
      "CARGO\n",
      "\"\"\"\n",
      "\n",
      "numCargo = 70\n",
      "\n",
      "cargo_size = 4\n",
      "\n",
      "cargo_station_destination = (\n",
      "\t4, #  0\n",
      "\t5, #  1\n",
      "\t4, #  2\n",
      "\t4, #  3\n",
      "\t4, #  4\n",
      "\t3, #  5\n",
      "\t4, #  6\n",
      "\t4, #  7\n",
      "\t3, #  8\n",
      "\t3, #  9\n",
      "\t3, #  10\n",
      "\t4, #  11\n",
      "\t5, #  12\n",
      "\t4, #  13\n",
      "\t4, #  14\n",
      "\t5, #  15\n",
      "\t4, #  16\n",
      "\t4, #  17\n",
      "\t3, #  18\n",
      "\t5, #  19\n",
      "\t5, #  20\n",
      "\t3, #  21\n",
      "\t4, #  22\n",
      "\t4, #  23\n",
      "\t5, #  24\n",
      "\t4, #  25\n",
      "\t5, #  26\n",
      "\t4, #  27\n",
      "\t3, #  28\n",
      "\t5, #  29\n",
      "\t5, #  30\n",
      "\t4, #  31\n",
      "\t4, #  32\n",
      "\t4, #  33\n",
      "\t3, #  34\n",
      "\t5, #  35\n",
      "\t4, #  36\n",
      "\t5, #  37\n",
      "\t3, #  38\n",
      "\t3, #\n",
      "\n",
      "Sample 3140\n",
      "Prompt: Generate Python code for: Gene Expression Omnibus: NCBI gene expression and hybridization array data repository\n",
      "Abstract: The Gene Expression Omnibus (GEO) project was initiated in response to the growing demand for a public repository for high-throughput gene expression data. GEO provides a flexible and open design that facilitates submission, storage and retrieval of heterogeneous data sets from high-throughput gene expression and genomic hybridization experiments. GEO is not intended to replace in house gene expre\n",
      "Code: \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "indices = random.sample(range(len(dataset)), 5)\n",
    "\n",
    "for i in indices:\n",
    "    sample = dataset[i]\n",
    "    print(f\"\\nSample {i + 1}\")\n",
    "    print(\"Prompt:\", sample[\"prompt\"])\n",
    "    print(\"Code:\", sample[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc5328-6246-4625-aa33-b4180430b9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f45ea2b-296b-49af-a18b-21d2e4057105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter base_model.model.transformer.wte.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.wpe.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.6.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.7.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.8.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.9.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.10.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.11.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.12.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.13.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.14.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.15.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.16.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.17.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.18.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.ln_1.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.ln_1.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.attn.c_attn.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.attn.c_attn.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.attn.c_attn.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.attn.c_attn.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.attn.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.attn.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.attn.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.attn.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.ln_2.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.ln_2.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.mlp.c_fc.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.mlp.c_fc.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.mlp.c_fc.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.mlp.c_fc.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.mlp.c_proj.base_layer.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.mlp.c_proj.base_layer.bias on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.mlp.c_proj.lora_A.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.h.19.mlp.c_proj.lora_B.default.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.ln_f.weight on wrong device: cuda:1\n",
      "Parameter base_model.model.transformer.ln_f.bias on wrong device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.device != device:\n",
    "        print(f\"Parameter {name} on wrong device: {param.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cacdb398-c14d-4609-9af4-19c260ef3375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (4.52.0.dev0)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers) (0.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from requests->transformers) (2025.6.15)\n",
      "Using cached transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.0.dev0\n",
      "    Uninstalling transformers-4.52.0.dev0:\n",
      "      Successfully uninstalled transformers-4.52.0.dev0\n",
      "Successfully installed transformers-4.53.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "824fb90a-4c13-4917-8a48-658f9c9a2d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitnet\n",
      "  Downloading bitnet-0.2.5-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torch in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from bitnet) (2.6.0)\n",
      "Collecting zetascale (from bitnet)\n",
      "  Downloading zetascale-2.8.6-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: filelock in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (4.14.1)\n",
      "Requirement already satisfied: networkx in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torch->bitnet) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from sympy==1.13.1->torch->bitnet) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from jinja2->torch->bitnet) (3.0.2)\n",
      "Requirement already satisfied: accelerate in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from zetascale->bitnet) (1.8.1)\n",
      "Collecting argparse<2.0.0,>=1.4.0 (from zetascale->bitnet)\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting beartype (from zetascale->bitnet)\n",
      "  Downloading beartype-0.21.0-py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: bitsandbytes in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from zetascale->bitnet) (0.46.1)\n",
      "Collecting colt5-attention (from zetascale->bitnet)\n",
      "  Downloading CoLT5_attention-0.11.1-py3-none-any.whl.metadata (737 bytes)\n",
      "Collecting einops-exts==0.0.4 (from zetascale->bitnet)\n",
      "  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
      "Collecting joblib<1.4.0,>=1.3.0 (from zetascale->bitnet)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting local-attention (from zetascale->bitnet)\n",
      "  Downloading local_attention-1.11.1-py3-none-any.whl.metadata (907 bytes)\n",
      "Collecting loguru (from zetascale->bitnet)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting rich (from zetascale->bitnet)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting scikit-learn<1.6.0,>=1.5.0 (from zetascale->bitnet)\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torchvision in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from zetascale->bitnet) (0.21.0+cu118)\n",
      "Requirement already satisfied: tqdm in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from zetascale->bitnet) (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.20.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from zetascale->bitnet) (4.53.1)\n",
      "Collecting vector-quantize-pytorch (from zetascale->bitnet)\n",
      "  Downloading vector_quantize_pytorch-1.22.17-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from scikit-learn<1.6.0,>=1.5.0->zetascale->bitnet) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from scikit-learn<1.6.0,>=1.5.0->zetascale->bitnet) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from scikit-learn<1.6.0,>=1.5.0->zetascale->bitnet) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers<5.0.0,>=4.20.0->zetascale->bitnet) (0.33.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers<5.0.0,>=4.20.0->zetascale->bitnet) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers<5.0.0,>=4.20.0->zetascale->bitnet) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers<5.0.0,>=4.20.0->zetascale->bitnet) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers<5.0.0,>=4.20.0->zetascale->bitnet) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers<5.0.0,>=4.20.0->zetascale->bitnet) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from transformers<5.0.0,>=4.20.0->zetascale->bitnet) (0.5.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers<5.0.0,>=4.20.0->zetascale->bitnet) (1.1.5)\n",
      "Requirement already satisfied: psutil in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from accelerate->zetascale->bitnet) (7.0.0)\n",
      "Collecting hyper-connections>=0.1.8 (from local-attention->zetascale->bitnet)\n",
      "  Downloading hyper_connections-0.2.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.20.0->zetascale->bitnet) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.20.0->zetascale->bitnet) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.20.0->zetascale->bitnet) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.20.0->zetascale->bitnet) (2025.6.15)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->zetascale->bitnet)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from rich->zetascale->bitnet) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->zetascale->bitnet)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages (from torchvision->zetascale->bitnet) (11.3.0)\n",
      "Collecting einx>=0.3.0 (from vector-quantize-pytorch->zetascale->bitnet)\n",
      "  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting frozendict (from einx>=0.3.0->vector-quantize-pytorch->zetascale->bitnet)\n",
      "  Downloading frozendict-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
      "Downloading bitnet-0.2.5-py3-none-any.whl (29 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading zetascale-2.8.6-py3-none-any.whl (533 kB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m533.8/533.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
      "Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading beartype-0.21.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading CoLT5_attention-0.11.1-py3-none-any.whl (18 kB)\n",
      "Downloading local_attention-1.11.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading hyper_connections-0.2.1-py3-none-any.whl (16 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading vector_quantize_pytorch-1.22.17-py3-none-any.whl (47 kB)\n",
      "Downloading einx-0.3.0-py3-none-any.whl (102 kB)\n",
      "Downloading frozendict-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "Installing collected packages: argparse, mdurl, loguru, joblib, frozendict, einops, beartype, scikit-learn, markdown-it-py, einx, einops-exts, rich, vector-quantize-pytorch, hyper-connections, local-attention, colt5-attention, zetascale, bitnet\n",
      "\u001b[2K  Attempting uninstall: joblib\n",
      "\u001b[2K    Found existing installation: joblib 1.5.1\n",
      "\u001b[2K    Uninstalling joblib-1.5.1:\n",
      "\u001b[2K      Successfully uninstalled joblib-1.5.1\n",
      "\u001b[2K  Attempting uninstall: scikit-learn[90mââââââââââââââââââââââââââ\u001b[0m \u001b[32m 6/18\u001b[0m [beartype]\n",
      "\u001b[2K    Found existing installation: scikit-learn 1.7.0âââââââââââ\u001b[0m \u001b[32m 6/18\u001b[0m [beartype]\n",
      "\u001b[2K    Uninstalling scikit-learn-1.7.0:ââââââââââââââââââââââââââ\u001b[0m \u001b[32m 6/18\u001b[0m [beartype]\n",
      "\u001b[2K      Successfully uninstalled scikit-learn-1.7.0âââââââââââââ\u001b[0m \u001b[32m 6/18\u001b[0m [beartype]\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m18/18\u001b[0m [bitnet]16/18\u001b[0m [zetascale]ections]torch]\n",
      "\u001b[1A\u001b[2KSuccessfully installed argparse-1.4.0 beartype-0.21.0 bitnet-0.2.5 colt5-attention-0.11.1 einops-0.8.1 einops-exts-0.0.4 einx-0.3.0 frozendict-2.4.6 hyper-connections-0.2.1 joblib-1.3.2 local-attention-1.11.1 loguru-0.7.3 markdown-it-py-3.0.0 mdurl-0.1.2 rich-14.0.0 scikit-learn-1.5.2 vector-quantize-pytorch-1.22.17 zetascale-2.8.6\n"
     ]
    }
   ],
   "source": [
    "!pip install bitnet einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2422f2ee-829a-447d-ab0b-82972f49351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (index: 0)\n",
      "Device map: {'': 0}\n",
      "trainable params: 1,740,800 || all params: 165,884,928 || trainable%: 1.0494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f401386014441dad07f53898e2d152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 02:24:57,816 - WARNING - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3987' max='3987' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3987/3987 5:37:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.382400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.338600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.266300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.277100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.320900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.220700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.226400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.162600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.191600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.249200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.226200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.214600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.229300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.191400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.248900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.186900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.175400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.226200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>1.136600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>1.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>1.166300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>1.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>1.107400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>1.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>1.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>1.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>1.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>1.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>1.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>1.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>1.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>1.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>1.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>1.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>1.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>1.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>1.269100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>1.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.102200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>1.185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>1.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>1.149300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>1.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>1.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>1.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>1.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>1.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>1.147400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>1.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>1.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>1.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>1.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>1.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>1.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>1.155400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>1.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>1.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>1.229500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>1.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>1.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>1.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>1.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>1.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>1.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>1.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>1.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>1.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>1.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>1.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>1.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>1.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.183600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>1.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>1.170900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>1.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>1.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>1.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>1.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>1.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>1.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>1.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>1.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>1.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>1.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>1.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>1.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>1.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>1.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>1.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>1.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>1.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>1.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>1.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>1.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>1.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>1.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>1.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>1.186300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>1.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>1.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>1.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>1.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>1.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>1.162200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Adapters saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model_name = \"bigcode/tiny_starcoder_py\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_index = torch.cuda.current_device() if device == \"cuda\" else None\n",
    "print(f\"Using device: {device} (index: {device_index})\")\n",
    "\n",
    "device_map = {\"\": device_index if device == \"cuda\" else device}\n",
    "print(f\"Device map: {device_map}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,  # Use fixed device mapping\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "def tokenize_func(examples):\n",
    "    combined = [f\"# {p}\\n{c}\" for p, c in zip(examples[\"prompt\"], examples[\"code\"])]\n",
    "    return tokenizer(\n",
    "        combined,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_func, \n",
    "    batched=True,\n",
    "    remove_columns=[\"prompt\", \"code\"]\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./scientific-codegen\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    report_to=\"none\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=0.3\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./codegen-lora-adapters\")\n",
    "tokenizer.save_pretrained(\"./codegen-lora-adapters\")\n",
    "print(\"Training complete! Adapters saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b24c3c-c11d-433f-94ae-4397a85e0495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (index: 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Write a Python function to calculate factorial\n",
      "import numpy as np\n",
      "\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "\n",
      "def factorial_prime(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial_prime(n-1)\n",
      "\n",
      "def factorial_prime_prime(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial_prime_prime(n-1)\n",
      "\n",
      "def factorial_prime_prime_prime(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial_prime_prime_prime(n-1)\n",
      "\n",
      "def factorial_prime_prime_prime_prime(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial_prime_prime\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model_name = \"bigcode/tiny_starcoder_py\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_index = torch.cuda.current_device() if device == \"cuda\" else None\n",
    "print(f\"Using device: {device} (index: {device_index})\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"./codegen-lora-adapters\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "prompt = \"# Write a Python function to calculate factorial\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.2,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab2fc8-e628-4e80-9522-716f5376a585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
