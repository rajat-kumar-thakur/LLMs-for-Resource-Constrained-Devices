{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644cf6e-edad-4cac-bc22-84e85736364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimization comparison with 15 test prompts\n",
      "Created 30 test prompts\n",
      "\n",
      "========================================\n",
      "Testing: Base Model\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Running Enhanced Smoke Tests\n",
      "==================================================\n",
      "\n",
      "Smoke Test 1: Generate a complete, self-contained Python code for text classification. Title: Pelvic Girdle Pain d...\n",
      "\n",
      "Generated Code:\n",
      "import numpy as np\n",
      "# Create synthetic data\n",
      "X = np.random.rand(100, 4)\n",
      "y = np.random.randint(0, 2, 100)\n",
      "\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.switch_backend('Agg')\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Load data\n",
      "df = pd.read_csv('data/pelvic-girdle-pain-during-or-after-pregnancy-a-review-of-recent-evidence-and-a-clinical-care-path-proposal.csv')\n",
      "\n",
      "# Split data\n",
      "X, y = df.iloc[:, 0:2], df.iloc[:, 2]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Create model\n",
      "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Predict\n",
      "y_pred = model.predict(X_test)\n",
      "\n",
      "# Print results\n",
      "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
      "\n",
      "# Visualize\n",
      "plt.plot(y_test, y_pred, 'r')\n",
      "plt.savefig('output.png')\n",
      "\n",
      "Execution Result:\n",
      "{'status': 'error', 'message': \"FileNotFoundError: [Errno 2] No such file or directory: 'data/pelvic-girdle-pain-during-or-after-pregnancy-a-review-of-recent-evidence-and-a-clinical-care-path-proposal.csv'\"}\n",
      "\n",
      "FULL CODE WITH ERROR:\n",
      "import numpy as np\n",
      "# Create synthetic data\n",
      "X = np.random.rand(100, 4)\n",
      "y = np.random.randint(0, 2, 100)\n",
      "\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.switch_backend('Agg')\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Load data\n",
      "df = pd.read_csv('data/pelvic-girdle-pain-during-or-after-pregnancy-a-review-of-recent-evidence-and-a-clinical-care-path-proposal.csv')\n",
      "\n",
      "# Split data\n",
      "X, y = df.iloc[:, 0:2], df.iloc[:, 2]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Create model\n",
      "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Predict\n",
      "y_pred = model.predict(X_test)\n",
      "\n",
      "# Print results\n",
      "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
      "\n",
      "# Visualize\n",
      "plt.plot(y_test, y_pred, 'r')\n",
      "plt.savefig('output.png')\n",
      "--------------------------------------------------\n",
      "\n",
      "Smoke Test 2: Generate a complete, self-contained Python code for text classification. Title: Packet Classificatio...\n",
      "\n",
      "Generated Code:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import numpy as np\n",
      "# Create synthetic data\n",
      "X = np.random.rand(100, 4)\n",
      "y = np.random.randint(0, 2, 100)\n",
      "\n",
      "import pandas as pd\n",
      "import sklearn\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Generate a synthetic dataset\n",
      "# 1. Generate a random sample of 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "Execution Result:\n",
      "{'status': 'success'}\n",
      "--------------------------------------------------\n",
      "\n",
      "Smoke Test 3: Generate a complete, self-contained Python code for text classification. Title: Bayesian Compressive...\n",
      "\n",
      "Generated Code:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.switch_backend('Agg')\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Generate synthetic data\n",
      "X = np.random.rand(1000, 10)\n",
      "y = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthe...\n",
      "\n",
      "Execution Result:\n",
      "{'status': 'error', 'message': \"NameError: name 'X_synth' is not defined\"}\n",
      "\n",
      "FULL CODE WITH ERROR:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.switch_backend('Agg')\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Generate synthetic data\n",
      "X = np.random.rand(1000, 10)\n",
      "y = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synthetic = np.random.rand(1000, 10)\n",
      "y_synthetic = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a synthetic dataset based on the abstract and implement a classification model\n",
      "X_synth\n",
      "--------------------------------------------------\n",
      "\n",
      "Smoke Test 4: Create a RandomForest classifier and show accuracy...\n",
      "\n",
      "Generated Code:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.switch_backend('Agg')\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Generate synthetic data\n",
      "X = np.random.rand(1000, 2)\n",
      "y = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a RandomForestClassifier\n",
      "rf = RandomForestClassifier(n_estimators=100, random_state=123)\n",
      "\n",
      "# Train the model\n",
      "rf.fit(X, y)\n",
      "\n",
      "# Print the model's accuracy\n",
      "print(rf.score(X, y))\n",
      "\n",
      "# Show the model's accuracy\n",
      "plt.plot(X, y, 'o')\n",
      "plt.plot(X, rf.predict(X), 'r')\n",
      "plt.savefig('output.png')\n",
      "\n",
      "Execution Result:\n",
      "{'status': 'error', 'message': 'ValueError: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.'}\n",
      "\n",
      "FULL CODE WITH ERROR:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.switch_backend('Agg')\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Generate synthetic data\n",
      "X = np.random.rand(1000, 2)\n",
      "y = np.random.rand(1000, 1)\n",
      "\n",
      "# Create a RandomForestClassifier\n",
      "rf = RandomForestClassifier(n_estimators=100, random_state=123)\n",
      "\n",
      "# Train the model\n",
      "rf.fit(X, y)\n",
      "\n",
      "# Print the model's accuracy\n",
      "print(rf.score(X, y))\n",
      "\n",
      "# Show the model's accuracy\n",
      "plt.plot(X, y, 'o')\n",
      "plt.plot(X, rf.predict(X), 'r')\n",
      "plt.savefig('output.png')\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  60%|██████████████████████████████████████████▌                            | 18/30 [05:38<03:45, 18.79s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  70%|█████████████████████████████████████████████████▋                     | 21/30 [06:34<02:48, 18.77s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 30/30 [09:24<00:00, 18.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing: Pruning\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  50%|███████████████████████████████████▌                                   | 15/30 [04:58<04:53, 19.54s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  70%|█████████████████████████████████████████████████▋                     | 21/30 [06:51<02:51, 19.01s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  73%|████████████████████████████████████████████████████                   | 22/30 [07:10<02:32, 19.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  77%|██████████████████████████████████████████████████████▍                | 23/30 [07:30<02:15, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cluster 1: [0.50670111 0.50224629 0.48357157 0.51931862 0.45603444 0.51805392\n",
      " 0.50348485 0.48355575 0.48547901 0.48230391 0.46579627 0.48085043\n",
      " 0.49917958 0.51608233 0.55675878 0.50889784 0.5027395  0.46246684\n",
      " 0.44249029 0.49109996 0.49078    0.51803542 0.4868742  0.44413671\n",
      " 0.50630479 0.49034262 0.5045502  0.50168443 0.50287687 0.52920788\n",
      " 0.48666926 0.49255272 0.44847509 0.50480451 0.5061533  0.50834318\n",
      " 0.4267994  0.48945894 0.50101083 0.51535873 0.54997231 0.44782594\n",
      " 0.51630592 0.48719913 0.57823812 0.56294441 0.5700564  0.49642837\n",
      " 0.47771048 0.4561513  0.46771844 0.51569986 0.42146168 0.44552444\n",
      " 0.47287754 0.4914742  0.49052169 0.50562655 0.46091732 0.49718929\n",
      " 0.48769634 0.52167781 0.4951209  0.51149464 0.5253621  0.52285975\n",
      " 0.48688125 0.43813436 0.43751669 0.50264748 0.46520519 0.50027994\n",
      " 0.49867802 0.50738294 0.47547913 0.48042824 0.48041331 0.48378078\n",
      " 0.48624253 0.47321851 0.54737698 0.55218129 0.50166451 0.55426822\n",
      " 0.4995175  0.55512668 0.4988276  0.44646204 0.48197672 0.52707338\n",
      " 0.43150768 0.51564279 0.47528058 0.56814453 0.51196498 0.50129686\n",
      " 0.51839468 0.53191276 0.5335495  0.4786838 ]\n",
      "Mean cluster 2: [0.50828827 0.52089527 0.47723592 0.47027628 0.52180672 0.4884092\n",
      " 0.46931471 0.5239274  0.50249671 0.53229459 0.5456051  0.5131596\n",
      " 0.50863475 0.49461338 0.44779608 0.48032268 0.50262425 0.50540606\n",
      " 0.55584338 0.51133234 0.49729298 0.47779796 0.5067369  0.53252612\n",
      " 0.51098488 0.48571857 0.50783186 0.52499914 0.51795931 0.45781547\n",
      " 0.49947151 0.51769369 0.52675908 0.48207344 0.47829823 0.49571022\n",
      " 0.54478166 0.51505087 0.53345929 0.47160144 0.4964332  0.55689671\n",
      " 0.46991631 0.48287826 0.44236891 0.46330076 0.45435996 0.47970216\n",
      " 0.54833639 0.54234066 0.53715554 0.48246409 0.59092081 0.57127755\n",
      " 0.51804827 0.51819354 0.49561916 0.50467635 0.52923121 0.49861184\n",
      " 0.53853143 0.49725218 0.50500298 0.47960413 0.48783828 0.45503823\n",
      " 0.51461592 0.57958243 0.56113881 0.47994912 0.53278752 0.49604727\n",
      " 0.50218518 0.48218608 0.53509138 0.51871285 0.52302123 0.52965513\n",
      " 0.50326406 0.52339911 0.46516967 0.46467927 0.51494918 0.44797746\n",
      " 0.49541037 0.43190241 0.47734499 0.54708083 0.51714466 0.47816651\n",
      " 0.55851975 0.47974025 0.49422478 0.44328918 0.49756141 0.51910715\n",
      " 0.51244847 0.48950123 0.47906113 0.49704675]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  80%|████████████████████████████████████████████████████████▊              | 24/30 [07:49<01:55, 19.17s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 30/30 [09:42<00:00, 19.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing: Quantization\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n",
      "Performance measurement failed: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   7%|████▊                                                                   | 2/30 [00:00<00:05,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  13%|█████████▌                                                              | 4/30 [00:00<00:05,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  20%|██████████████▍                                                         | 6/30 [00:01<00:04,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  27%|███████████████████▏                                                    | 8/30 [00:01<00:04,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  30%|█████████████████████▌                                                  | 9/30 [00:01<00:04,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  37%|██████████████████████████                                             | 11/30 [00:02<00:04,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  43%|██████████████████████████████▊                                        | 13/30 [00:02<00:03,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  47%|█████████████████████████████████▏                                     | 14/30 [00:02<00:03,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  50%|███████████████████████████████████▌                                   | 15/30 [00:03<00:03,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  57%|████████████████████████████████████████▏                              | 17/30 [00:03<00:02,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  63%|████████████████████████████████████████████▉                          | 19/30 [00:03<00:02,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  70%|█████████████████████████████████████████████████▋                     | 21/30 [00:04<00:01,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  77%|██████████████████████████████████████████████████████▍                | 23/30 [00:04<00:01,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  83%|███████████████████████████████████████████████████████████▏           | 25/30 [00:05<00:00,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  87%|█████████████████████████████████████████████████████████████▌         | 26/30 [00:05<00:00,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  97%|████████████████████████████████████████████████████████████████████▋  | 29/30 [00:05<00:00,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "Generation error: 'function' object has no attribute 'dtype'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 30/30 [00:06<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation error: 'function' object has no attribute 'dtype'\n",
      "\n",
      "========================================\n",
      "Testing: Weight Sharing\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 30/30 [09:31<00:00, 19.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Optimization Results\n",
      "================================================================================\n",
      "     Technique Latency (ms) Throughput (samples/s) Memory (MB) Inference Success (%) Valid Code (%) Execution Success (%) Quality Score\n",
      "    Base Model      8030.85                   0.12      8422.1                 100.0           53.3                  43.8          72.7\n",
      "       Pruning      8068.14                   0.12      8422.1                 100.0           50.0                  46.7          68.0\n",
      "  Quantization         0.00                   0.00         0.0                   0.0            0.0                   N/A           0.0\n",
      "Weight Sharing      8204.35                   0.12      8172.1                 100.0            0.0                   N/A          40.0\n",
      "\n",
      "Results saved to optimization_results.csv\n",
      "Visualization saved to optimization_results.png\n",
      "\n",
      "Test pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.quantization import quantize_dynamic\n",
    "import sklearn\n",
    "import types\n",
    "import tempfile\n",
    "\n",
    "CPU_MODE = False\n",
    "TEST_SAMPLE_SIZE = 15\n",
    "OPTIMIZATION_TECHNIQUES = [\n",
    "    \"Base Model\",\n",
    "    \"Pruning\",\n",
    "    \"Quantization\",\n",
    "    \"Weight Sharing\"\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./final-model\")\n",
    "\n",
    "def load_test_datasets():\n",
    "    test_prompts = []\n",
    "    try:\n",
    "        scidocs_path = \"scidocs_data\"\n",
    "        os.makedirs(scidocs_path, exist_ok=True)\n",
    "        if not os.path.exists(os.path.join(scidocs_path, \"paper_metadata_view_cite_read.json\")):\n",
    "            subprocess.run([\n",
    "                \"aws\", \"s3\", \"sync\", \"--no-sign-request\",\n",
    "                \"s3://ai2-s2-research-public/specter/scidocs/\",\n",
    "                scidocs_path, \"--region\", \"us-west-2\", \"--quiet\"\n",
    "            ], check=True)\n",
    "        with open(os.path.join(scidocs_path, \"paper_metadata_view_cite_read.json\"), \"r\") as f:\n",
    "            scidocs_data = json.load(f)\n",
    "        for i, (paper_id, content) in enumerate(scidocs_data.items()):\n",
    "            if i >= 5:\n",
    "                break\n",
    "            title = content.get('title', '') or ''\n",
    "            abstract = content.get('abstract', '') or ''\n",
    "            if len(title) > 10 and len(abstract) > 200:\n",
    "                test_prompts.append({\n",
    "                    \"text\": (\n",
    "                        f\"Generate a complete, self-contained Python code for text classification. \"\n",
    "                        f\"Title: {title}\\n\"\n",
    "                        f\"Abstract: {abstract[:400]}\\n\"\n",
    "                        \"Create a synthetic dataset based on the abstract and implement a classification model.\"\n",
    "                    ),\n",
    "                    \"source\": \"scidocs\",\n",
    "                    \"type\": \"classification\"\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"SciDocs loading failed: {str(e)}\")\n",
    "    try:\n",
    "        astronomy = load_dataset(\"David-Xu/astronomy-stack-dpo-text\", split=\"train\")\n",
    "        for i, example in enumerate(astronomy):\n",
    "            if i >= 5:\n",
    "                break\n",
    "            test_prompts.append({\n",
    "                \"text\": (\n",
    "                    \"Generate a complete, self-contained Python code to solve this astronomy problem:\\n\"\n",
    "                    f\"{example['prompt']}\\n\"\n",
    "                    \"Create any necessary synthetic data and implement a solution.\"\n",
    "                ),\n",
    "                \"source\": \"astronomy\",\n",
    "                \"type\": \"problem_solving\"\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Astronomy dataset loading failed: {str(e)}\")\n",
    "    try:\n",
    "        science = load_dataset(\"millawell/wikipedia_field_of_science\", split=\"train\")\n",
    "        for i, example in enumerate(science):\n",
    "            if i >= 5:\n",
    "                break\n",
    "            test_prompts.append({\n",
    "                \"text\": (\n",
    "                    \"Generate a complete, self-contained Python code for scientific text classification:\\n\"\n",
    "                    f\"Text: {example['text']}\\n\"\n",
    "                    \"Create a synthetic dataset and implement a classification model.\"\n",
    "                ),\n",
    "                \"source\": \"wikipedia_science\",\n",
    "                \"type\": \"classification\"\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Science dataset loading failed: {str(e)}\")\n",
    "    for i in range(5):\n",
    "        X, y = make_classification(\n",
    "            n_samples=100, \n",
    "            n_features=4, \n",
    "            n_informative=2, \n",
    "            n_classes=2,\n",
    "            random_state=i\n",
    "        )\n",
    "        data = pd.DataFrame(X, columns=[f\"feature_{j}\" for j in range(4)])\n",
    "        data[\"target\"] = y\n",
    "        test_prompts.append({\n",
    "            \"text\": \"Create a RandomForest classifier and show accuracy\",\n",
    "            \"data\": data,\n",
    "            \"source\": \"synthetic\",\n",
    "            \"type\": \"classification\"\n",
    "        })\n",
    "        X, y = make_blobs(n_samples=100, centers=3, cluster_std=1.5, random_state=i)\n",
    "        data = pd.DataFrame(X, columns=[\"x\", \"y\"])\n",
    "        test_prompts.append({\n",
    "            \"text\": \"Perform K-means clustering on this data\",\n",
    "            \"data\": data,\n",
    "            \"source\": \"synthetic\",\n",
    "            \"type\": \"clustering\"\n",
    "        })\n",
    "        X, _ = make_blobs(n_samples=100, centers=3, cluster_std=1.5, random_state=i)\n",
    "        outliers = np.random.uniform(low=-10, high=10, size=(5, 2))\n",
    "        X = np.vstack([X, outliers])\n",
    "        data = pd.DataFrame(X, columns=[\"x\", \"y\"])\n",
    "        test_prompts.append({\n",
    "            \"text\": \"Detect anomalies using Isolation Forest\",\n",
    "            \"data\": data,\n",
    "            \"source\": \"synthetic\",\n",
    "            \"type\": \"outlier_detection\"\n",
    "        })\n",
    "    print(f\"Created {len(test_prompts)} test prompts\")\n",
    "    return test_prompts\n",
    "\n",
    "def apply_optimization(technique_name):\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"./final-model\")\n",
    "        if technique_name == \"Pruning\":\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear) and \"lora\" not in name.lower():\n",
    "                    try:\n",
    "                        prune.l1_unstructured(module, name='weight', amount=0.1)\n",
    "                        prune.remove(module, 'weight')\n",
    "                    except:\n",
    "                        continue\n",
    "            return model\n",
    "        if technique_name == \"Quantization\":\n",
    "            model = model.cpu()\n",
    "            return quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "        if technique_name == \"Weight Sharing\":\n",
    "            if hasattr(model, 'lm_head') and hasattr(model, 'model'):\n",
    "                if hasattr(model.model, 'embed_tokens'):\n",
    "                    try:\n",
    "                        model.lm_head.weight = model.model.embed_tokens.weight\n",
    "                    except:\n",
    "                        pass\n",
    "            return model\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying {technique_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def generate_robust_code(generator, prompt_text, task_type):\n",
    "    if task_type == \"classification\":\n",
    "        task_instructions = \"Focus on classification using RandomForestClassifier. Create synthetic data if needed.\"\n",
    "    elif task_type == \"clustering\":\n",
    "        task_instructions = \"Use KMeans clustering and visualize results with matplotlib.\"\n",
    "    elif task_type == \"outlier_detection\":\n",
    "        task_instructions = \"Use IsolationForest for outlier detection. Highlight anomalies in visualization.\"\n",
    "    elif task_type == \"problem_solving\":\n",
    "        task_instructions = \"Solve the problem using appropriate scientific computing techniques.\"\n",
    "    else:\n",
    "        task_instructions = \"Solve the problem efficiently with appropriate algorithms.\"\n",
    "    structured_prompt = f\"\"\"\n",
    "Generate complete, self-contained Python code to solve this task:\n",
    "{prompt_text}\n",
    "\n",
    "Specific Requirements:\n",
    "1. Create any necessary synthetic data if not provided\n",
    "2. Use only numpy, pandas, sklearn and matplotlib\n",
    "3. {task_instructions}\n",
    "4. Create complete, runnable code\n",
    "5. Print results clearly\n",
    "6. For visualizations, use plt.savefig('output.png') instead of plt.show()\n",
    "7. Ensure the code is syntactically correct\n",
    "\n",
    "Code:\n",
    "```python\n",
    "\"\"\"\n",
    "    try:\n",
    "        output = generator(\n",
    "            structured_prompt,\n",
    "            temperature=0.1,\n",
    "            max_new_tokens=700,\n",
    "            truncation=True,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        return output[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def validate_code(generated_code):\n",
    "    if not generated_code:\n",
    "        return \"\"\n",
    "    if \"```python\" in generated_code:\n",
    "        generated_code = generated_code.split(\"```python\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in generated_code:\n",
    "        generated_code = generated_code.split(\"```\")[1].split(\"```\")[0]\n",
    "    repairs = [\n",
    "        (r\"from sklearn\\.\\w+ import \\*\", \"\"),\n",
    "        (r\"fit\\(\\)\", \"fit(X_train, y_train)\"),\n",
    "        (r\"predict\\(\\)\", \"predict(X_test)\"),\n",
    "        (r\"plt\\.show\\(\\)\", \"plt.savefig('output.png')\"),\n",
    "        (r\"import matplotlib\\.pyplot as plt\", \"import matplotlib.pyplot as plt\\nplt.switch_backend('Agg')\"),\n",
    "        (r\"\\.to_csv\\('data\\.csv'\\)\", \"\")\n",
    "    ]\n",
    "    for pattern, replacement in repairs:\n",
    "        generated_code = re.sub(pattern, replacement, generated_code)\n",
    "    required_imports = [\n",
    "        \"import numpy as np\",\n",
    "        \"import pandas as pd\",\n",
    "        \"import matplotlib.pyplot as plt\"\n",
    "    ]\n",
    "    for imp in required_imports:\n",
    "        if imp not in generated_code:\n",
    "            generated_code = imp + \"\\n\" + generated_code\n",
    "    if \"from sklearn\" not in generated_code and \"import sklearn\" not in generated_code:\n",
    "        generated_code = \"from sklearn.ensemble import RandomForestClassifier, IsolationForest\\n\" + \\\n",
    "                         \"from sklearn.cluster import KMeans\\n\" + \\\n",
    "                         \"from sklearn.model_selection import train_test_split\\n\" + \\\n",
    "                         \"from sklearn.metrics import accuracy_score, classification_report\\n\" + generated_code\n",
    "    if \"pd.DataFrame\" not in generated_code and \"X =\" not in generated_code:\n",
    "        synthetic_data = \"\\n# Create synthetic data\\nX = np.random.rand(100, 4)\\ny = np.random.randint(0, 2, 100)\\n\"\n",
    "        generated_code = generated_code.replace(\"import numpy as np\", \"import numpy as np\" + synthetic_data, 1)\n",
    "    return generated_code.strip()\n",
    "\n",
    "def safe_execute(code: str, data=None):\n",
    "    if not code:\n",
    "        return {\"status\": \"error\", \"message\": \"Empty code\"}\n",
    "    safe_env = {\n",
    "        \"__builtins__\": {\n",
    "            'print': print, 'range': range, 'len': len, 'str': str, 'int': int,\n",
    "            'float': float, 'bool': bool, 'list': list, 'dict': dict, 'tuple': tuple,\n",
    "            'set': set, 'min': min, 'max': max, 'sum': sum, 'abs': abs, 'round': round,\n",
    "            'enumerate': enumerate, 'zip': zip, '__import__': __import__\n",
    "        },\n",
    "        \"np\": np,\n",
    "        \"pd\": pd,\n",
    "        \"plt\": plt,\n",
    "        \"sklearn\": sklearn,\n",
    "        \"RandomForestClassifier\": RandomForestClassifier,\n",
    "        \"IsolationForest\": IsolationForest,\n",
    "        \"KMeans\": KMeans,\n",
    "        \"train_test_split\": train_test_split,\n",
    "        \"accuracy_score\": accuracy_score,\n",
    "        \"classification_report\": classification_report,\n",
    "    }\n",
    "    if data is not None:\n",
    "        safe_env[\"data\"] = data\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        exec(code, safe_env)\n",
    "        return {\"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"{type(e).__name__}: {str(e)}\"}\n",
    "\n",
    "def measure_inference_performance(generator, prompt_text, num_runs=3):\n",
    "    metrics = {\n",
    "        \"avg_latency\": 0,\n",
    "        \"throughput\": 0,\n",
    "        \"memory_usage\": 0,\n",
    "        \"success_rate\": 0\n",
    "    }\n",
    "    successes = 0\n",
    "    latencies = []\n",
    "    try:\n",
    "        _ = generator(prompt_text, max_new_tokens=50, truncation=True)\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            try:\n",
    "                run_start = time.time()\n",
    "                output = generator(\n",
    "                    prompt_text,\n",
    "                    max_new_tokens=300,\n",
    "                    truncation=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                latencies.append(time.time() - run_start)\n",
    "                successes += 1\n",
    "            except Exception:\n",
    "                continue\n",
    "        metrics[\"avg_latency\"] = np.mean(latencies) * 1000 if latencies else 0\n",
    "        metrics[\"throughput\"] = successes / max(0.001, time.time() - start_time)\n",
    "        metrics[\"success_rate\"] = successes / num_runs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            metrics[\"memory_usage\"] = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "        else:\n",
    "            metrics[\"memory_usage\"] = 0\n",
    "    except Exception as e:\n",
    "        print(f\"Performance measurement failed: {str(e)}\")\n",
    "    return metrics\n",
    "\n",
    "def evaluate_code_quality(generated_code):\n",
    "    if not generated_code:\n",
    "        return {\n",
    "            \"has_imports\": False,\n",
    "            \"has_model\": False,\n",
    "            \"has_print\": False,\n",
    "            \"is_runnable\": False,\n",
    "            \"score\": 0\n",
    "        }\n",
    "    has_imports = any(keyword in generated_code for keyword in\n",
    "                      [\"import numpy\", \"import pandas\", \"import sklearn\"])\n",
    "    has_model = any(keyword in generated_code for keyword in\n",
    "                    [\"RandomForest\", \"IsolationForest\", \"KMeans\"])\n",
    "    has_print = \"print(\" in generated_code\n",
    "    has_visualization = \"plt.savefig\" in generated_code or \"plt.plot\" in generated_code\n",
    "    has_data = \"X =\" in generated_code or \"pd.DataFrame\" in generated_code\n",
    "    is_runnable = has_imports and has_model and has_print and has_data\n",
    "    score = sum([has_imports, has_model, has_print, is_runnable, has_visualization]) / 5\n",
    "    return {\n",
    "        \"has_imports\": has_imports,\n",
    "        \"has_model\": has_model,\n",
    "        \"has_print\": has_print,\n",
    "        \"has_visualization\": has_visualization,\n",
    "        \"has_data\": has_data,\n",
    "        \"is_runnable\": is_runnable,\n",
    "        \"score\": score\n",
    "    }\n",
    "\n",
    "def run_smoke_tests(generator, test_prompts):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Running Enhanced Smoke Tests\")\n",
    "    print(\"=\"*50)\n",
    "    smoke_prompts = []\n",
    "    for prompt in test_prompts:\n",
    "        if prompt[\"source\"] in [\"scidocs\", \"astronomy\", \"wikipedia_science\"]:\n",
    "            smoke_prompts.append(prompt)\n",
    "            if len(smoke_prompts) >= 3:\n",
    "                break\n",
    "    for prompt in test_prompts:\n",
    "        if prompt[\"source\"] == \"synthetic\":\n",
    "            smoke_prompts.append(prompt)\n",
    "            break\n",
    "    for i, item in enumerate(smoke_prompts):\n",
    "        print(f\"\\nSmoke Test {i+1}: {item['text'][:100]}...\")\n",
    "        generated = generate_robust_code(generator, item[\"text\"], item.get(\"type\", \"\"))\n",
    "        code = validate_code(generated)\n",
    "        print(\"\\nGenerated Code:\")\n",
    "        print(code[:1000] + \"...\" if len(code) > 1000 else code)\n",
    "        data = item.get(\"data\", None)\n",
    "        exec_result = safe_execute(code, data)\n",
    "        print(\"\\nExecution Result:\")\n",
    "        print(exec_result)\n",
    "        if exec_result[\"status\"] == \"error\":\n",
    "            print(\"\\nFULL CODE WITH ERROR:\")\n",
    "            print(code)\n",
    "        if os.path.exists(\"output.png\"):\n",
    "            print(\"Visualization created: output.png\")\n",
    "            os.remove(\"output.png\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "def run_test_pipeline():\n",
    "    test_prompts = load_test_datasets()\n",
    "    results = {}\n",
    "    for technique in OPTIMIZATION_TECHNIQUES:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Testing: {technique}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        model = apply_optimization(technique)\n",
    "        if model is None:\n",
    "            print(f\"Skipping {technique} due to initialization error\")\n",
    "            continue\n",
    "        model.eval()\n",
    "        device = 0 if torch.cuda.is_available() and not CPU_MODE else -1\n",
    "        if technique == \"Quantization\":\n",
    "            device = -1\n",
    "        print(f\"Using device: {'GPU' if device >= 0 else 'CPU'}\")\n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device\n",
    "        )\n",
    "        if technique == \"Base Model\":\n",
    "            run_smoke_tests(generator, test_prompts)\n",
    "        tech_results = {\n",
    "            \"inference\": {\n",
    "                \"avg_latency\": 0,\n",
    "                \"throughput\": 0,\n",
    "                \"memory_usage\": 0,\n",
    "                \"success_rate\": 0\n",
    "            },\n",
    "            \"quality\": {\n",
    "                \"syntax_errors\": 0,\n",
    "                \"execution_errors\": 0,\n",
    "                \"valid_count\": 0,\n",
    "                \"quality_score\": 0,\n",
    "                \"scores\": []\n",
    "            }\n",
    "        }\n",
    "        if test_prompts:\n",
    "            perf_metrics = measure_inference_performance(generator, test_prompts[0][\"text\"])\n",
    "            tech_results[\"inference\"] = perf_metrics\n",
    "        for item in tqdm(test_prompts, desc=\"Testing\"):\n",
    "            try:\n",
    "                generated = generate_robust_code(generator, item[\"text\"], item.get(\"type\", \"\"))\n",
    "                code = validate_code(generated)\n",
    "                if not code:\n",
    "                    tech_results[\"quality\"][\"syntax_errors\"] += 1\n",
    "                    tech_results[\"quality\"][\"scores\"].append(0)\n",
    "                    continue\n",
    "                quality_metrics = evaluate_code_quality(code)\n",
    "                quality_score = quality_metrics[\"score\"]\n",
    "                tech_results[\"quality\"][\"scores\"].append(quality_score)\n",
    "                if not quality_metrics[\"is_runnable\"]:\n",
    "                    tech_results[\"quality\"][\"syntax_errors\"] += 1\n",
    "                    continue\n",
    "                tech_results[\"quality\"][\"valid_count\"] += 1\n",
    "                data = item.get(\"data\", None)\n",
    "                if data is not None:\n",
    "                    exec_result = safe_execute(code, data)\n",
    "                    if exec_result[\"status\"] == \"error\":\n",
    "                        tech_results[\"quality\"][\"execution_errors\"] += 1\n",
    "            except Exception as e:\n",
    "                tech_results[\"quality\"][\"syntax_errors\"] += 1\n",
    "                tech_results[\"quality\"][\"scores\"].append(0)\n",
    "                print(f\"Error during testing: {str(e)}\")\n",
    "        if tech_results[\"quality\"][\"scores\"]:\n",
    "            tech_results[\"quality\"][\"quality_score\"] = np.mean(tech_results[\"quality\"][\"scores\"])\n",
    "        else:\n",
    "            tech_results[\"quality\"][\"quality_score\"] = 0\n",
    "        results[technique] = tech_results\n",
    "    return results\n",
    "\n",
    "def present_results(results):\n",
    "    table_data = []\n",
    "    for tech, metrics in results.items():\n",
    "        inf = metrics[\"inference\"]\n",
    "        qual = metrics[\"quality\"]\n",
    "        valid_count = qual[\"valid_count\"]\n",
    "        total_tests = len(qual[\"scores\"]) if \"scores\" in qual else TEST_SAMPLE_SIZE\n",
    "        table_data.append({\n",
    "            \"Technique\": tech,\n",
    "            \"Latency (ms)\": f\"{inf['avg_latency']:.2f}\",\n",
    "            \"Throughput (samples/s)\": f\"{inf['throughput']:.2f}\",\n",
    "            \"Memory (MB)\": f\"{inf['memory_usage']:.1f}\",\n",
    "            \"Inference Success (%)\": f\"{inf['success_rate'] * 100:.1f}\",\n",
    "            \"Valid Code (%)\": f\"{valid_count / total_tests * 100:.1f}\" if total_tests > 0 else \"N/A\",\n",
    "            \"Execution Success (%)\": f\"{(1 - qual['execution_errors'] / max(1, valid_count)) * 100:.1f}\" if valid_count > 0 else \"N/A\",\n",
    "            \"Quality Score\": f\"{qual['quality_score'] * 100:.1f}\"\n",
    "        })\n",
    "    results_df = pd.DataFrame(table_data)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Optimization Results\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df.to_string(index=False))\n",
    "    results_df.to_csv(\"optimization_results.csv\", index=False)\n",
    "    print(\"\\nResults saved to optimization_results.csv\")\n",
    "    if not results_df.empty:\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        results_df[\"Latency Value\"] = results_df[\"Latency (ms)\"].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "        results_df[\"Valid Code Value\"] = results_df[\"Valid Code (%)\"].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "        results_df[\"Exec Success Value\"] = results_df[\"Execution Success (%)\"].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "        results_df[\"Quality Value\"] = results_df[\"Quality Score\"].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "        results_df.plot.bar(x=\"Technique\", y=\"Latency Value\", ax=ax[0, 0], legend=False, color='skyblue')\n",
    "        ax[0, 0].set_title('Inference Latency')\n",
    "        ax[0, 0].set_ylabel('Milliseconds')\n",
    "        results_df.plot.bar(x=\"Technique\", y=\"Valid Code Value\", ax=ax[0, 1], legend=False, color='lightgreen')\n",
    "        ax[0, 1].set_title('Valid Code Rate')\n",
    "        ax[0, 1].set_ylabel('Percentage')\n",
    "        results_df.plot.bar(x=\"Technique\", y=\"Exec Success Value\", ax=ax[1, 0], legend=False, color='salmon')\n",
    "        ax[1, 0].set_title('Execution Success Rate')\n",
    "        ax[1, 0].set_ylabel('Percentage')\n",
    "        results_df.plot.bar(x=\"Technique\", y=\"Quality Value\", ax=ax[1, 1], legend=False, color='purple')\n",
    "        ax[1, 1].set_title('Code Quality Score')\n",
    "        ax[1, 1].set_ylabel('Score (0-100)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('optimization_results.png', dpi=150)\n",
    "        print(\"Visualization saved to optimization_results.png\")\n",
    "        plt.close()\n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting optimization comparison with {TEST_SAMPLE_SIZE} test prompts\")\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    results = run_test_pipeline()\n",
    "    results_df = present_results(results)\n",
    "    print(\"\\nTest pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1e036-13c5-47ec-94dc-fb1c9b62b089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results will be saved in: optimization_results_20250708_183526\n",
      "Starting comprehensive optimization comparison with 100 test prompts\n",
      "Created 149 test prompts\n",
      "\n",
      "========================================\n",
      "Testing: Base Model\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Testing:   4%|██▊                                                                    | 6/149 [01:50<44:04, 18.50s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Testing:  69%|███████████████████████████████████████████████▋                     | 103/149 [32:42<14:21, 18.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cluster centers:\n",
      "[[0.47175284 0.51435111 0.4663811  0.52378434 0.50113873 0.48556265\n",
      "  0.54970838 0.47680876 0.51478626 0.46740017 0.52251781 0.49635192\n",
      "  0.52286201 0.51306513 0.52677907 0.55809872 0.50180197 0.52436608\n",
      "  0.55606249 0.51250068 0.4980419  0.47655803 0.5036204  0.5220877\n",
      "  0.45410017 0.50189981 0.4387408  0.447872   0.51322152 0.52861912\n",
      "  0.48961133 0.51623102 0.4902062  0.50374731 0.48919694 0.49882175\n",
      "  0.49168513 0.44515221 0.54864569 0.51890876 0.50525958 0.46759069\n",
      "  0.53186059 0.55457609 0.4595122  0.50132009 0.51435303 0.41324168\n",
      "  0.49440404 0.5055518  0.50436049 0.51856344 0.45662282 0.53559587\n",
      "  0.56385899 0.55069084 0.4843733  0.50887487 0.46860672 0.5024037\n",
      "  0.47646654 0.51472214 0.56397182 0.48614216 0.49514596 0.45810999\n",
      "  0.53292008 0.50017321 0.56608129 0.48712737 0.42363461 0.54237659\n",
      "  0.51497159 0.532301   0.5270013  0.51251087 0.53205925 0.45781093\n",
      "  0.47321491 0.48379425 0.49692288 0.55970748 0.43741463 0.40957281\n",
      "  0.51040471 0.57020577 0.54352824 0.50394982 0.49883675 0.540535\n",
      "  0.5261602  0.49392156 0.59075747 0.49213156 0.49393605 0.51688596\n",
      "  0.50804474 0.53062318 0.50630585 0.51480522]\n",
      " [0.52414263 0.47616297 0.52839245 0.49663113 0.50093449 0.51734022\n",
      "  0.48275569 0.51952136 0.48599605 0.50971005 0.47659161 0.48817852\n",
      "  0.4839042  0.46700791 0.49064608 0.44553518 0.46517749 0.50549903\n",
      "  0.44502493 0.50000575 0.5002543  0.52448163 0.50025033 0.49791308\n",
      "  0.52768245 0.49065742 0.52751108 0.52667205 0.49280547 0.46798668\n",
      "  0.49992246 0.45703546 0.49456343 0.51304624 0.49479889 0.50980055\n",
      "  0.51197349 0.52605913 0.44410029 0.49335556 0.51753179 0.51950448\n",
      "  0.5004293  0.47261055 0.49435572 0.52407654 0.49109604 0.57653856\n",
      "  0.51401988 0.4872801  0.49580442 0.49187797 0.52818932 0.49465446\n",
      "  0.44777326 0.4730862  0.49461126 0.46755432 0.5089166  0.49449408\n",
      "  0.52557148 0.44360637 0.47750646 0.47478805 0.52074721 0.54302167\n",
      "  0.47615628 0.48916506 0.45053207 0.49942483 0.54710089 0.46748695\n",
      "  0.46988223 0.43701612 0.50325338 0.50366067 0.47490257 0.52576812\n",
      "  0.53695377 0.49811935 0.48634534 0.45579462 0.55133569 0.55157517\n",
      "  0.46984855 0.4364128  0.48276172 0.50304659 0.51025238 0.46947967\n",
      "  0.46263371 0.4863825  0.44959212 0.52531022 0.49583564 0.52916888\n",
      "  0.49504137 0.4821457  0.49027317 0.52971262]]\n",
      "Mean cluster labels:\n",
      "[1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0\n",
      " 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1\n",
      " 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 0\n",
      " 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 1\n",
      " 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1\n",
      " 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0\n",
      " 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 1\n",
      " 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
      " 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1\n",
      " 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0\n",
      " 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1\n",
      " 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1\n",
      " 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 0\n",
      " 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0\n",
      " 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1\n",
      " 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0\n",
      " 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1\n",
      " 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1\n",
      " 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0\n",
      " 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1\n",
      " 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
      " 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1\n",
      " 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
      " 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  72%|█████████████████████████████████████████████████▌                   | 107/149 [33:58<13:06, 18.72s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  78%|█████████████████████████████████████████████████████▋               | 116/149 [36:45<10:11, 18.52s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  80%|███████████████████████████████████████████████████████              | 119/149 [37:40<09:17, 18.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1.4081342246764368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  84%|█████████████████████████████████████████████████████████▉           | 125/149 [39:35<07:35, 18.98s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  87%|████████████████████████████████████████████████████████████▏        | 130/149 [41:08<05:55, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means clustering results:\n",
      "Cluster 0: [0.58581833 0.37135601 0.303774   0.58579283 0.5239632  0.54581301\n",
      " 0.46492851 0.45370829 0.47399612 0.4677694 ]\n",
      "Cluster 1: [0.43740248 0.5831489  0.7579149  0.34255453 0.40161917 0.41156228\n",
      " 0.57429652 0.53362704 0.45437606 0.57663641]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  88%|████████████████████████████████████████████████████████████▋        | 131/149 [41:27<05:36, 18.72s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  93%|████████████████████████████████████████████████████████████████▎    | 139/149 [43:58<03:12, 19.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means clustering results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  96%|██████████████████████████████████████████████████████████████████▏  | 143/149 [45:12<01:52, 18.68s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing: 100%|█████████████████████████████████████████████████████████████████████| 149/149 [47:10<00:00, 19.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing: Pruning\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  60%|█████████████████████████████████████████▊                            | 89/149 [28:19<19:02, 19.04s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  61%|██████████████████████████████████████████▊                           | 91/149 [28:58<18:34, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cluster 1: [0.52732336 0.45985468 0.45712897 0.48160642 0.46292294 0.46294736\n",
      " 0.53930745 0.50201346 0.50265143 0.50964117 0.49299216 0.52483649\n",
      " 0.54394763 0.51523129 0.52185377 0.45640065 0.49294294 0.54693917\n",
      " 0.50566728 0.47113967 0.56794936 0.48639048 0.54246779 0.53987011\n",
      " 0.53768582 0.49649621 0.50441464 0.4168353  0.50925989 0.46224085\n",
      " 0.43904501 0.48992033 0.52363608 0.47523084 0.49944347 0.50495728\n",
      " 0.51111161 0.49196451 0.51327946 0.49930577 0.54396564 0.48447745\n",
      " 0.50751656 0.51488558 0.50173194 0.52058781 0.52450917 0.48625168\n",
      " 0.48373974 0.46937661 0.43889769 0.5618763  0.5052914  0.53878447\n",
      " 0.48502037 0.49318434 0.44208701 0.49287475 0.52239023 0.49357236\n",
      " 0.49618717 0.45956227 0.49419739 0.48784295 0.49810573 0.49953436\n",
      " 0.48786942 0.49196092 0.50162665 0.40086148 0.49623579 0.49330577\n",
      " 0.55990727 0.4676051  0.54947615 0.54163637 0.50921208 0.48422713\n",
      " 0.54843852 0.49771772 0.53071599 0.48388473 0.51435339 0.50039847\n",
      " 0.5185499  0.50423751 0.54868315 0.43349956 0.49001579 0.51835896\n",
      " 0.49329619 0.50741524 0.45490386 0.50031239 0.46305435 0.41598839\n",
      " 0.49272587 0.51949684 0.48327852 0.47835402]\n",
      "Mean cluster 2: [0.4919851  0.52740628 0.53718835 0.52728922 0.54136007 0.53281623\n",
      " 0.47673322 0.51601042 0.47242111 0.47970402 0.50907176 0.49222023\n",
      " 0.47275704 0.48606998 0.48474018 0.55578017 0.49584537 0.47569557\n",
      " 0.48884753 0.54313092 0.43829965 0.54033273 0.46614713 0.44598611\n",
      " 0.47929484 0.53405138 0.49611635 0.55336685 0.4799447  0.5348043\n",
      " 0.58491845 0.5243363  0.49804066 0.51146042 0.4858911  0.49350471\n",
      " 0.48572432 0.49606034 0.46707694 0.49873293 0.48493859 0.49137875\n",
      " 0.49512243 0.49243775 0.50429048 0.48993953 0.469527   0.51493011\n",
      " 0.51061672 0.55090598 0.57171448 0.4569701  0.51204157 0.47765102\n",
      " 0.51003929 0.54040859 0.55773902 0.49906451 0.48619183 0.50976068\n",
      " 0.4950902  0.54989036 0.4815937  0.52793801 0.52427811 0.51992285\n",
      " 0.50600373 0.49210388 0.51606447 0.56175358 0.52670043 0.54176071\n",
      " 0.43993265 0.5382716  0.47553448 0.47447994 0.4984562  0.51058626\n",
      " 0.46929878 0.49471159 0.48723231 0.51894876 0.4719117  0.48458357\n",
      " 0.49710508 0.50679669 0.46037327 0.57483609 0.52061232 0.49719455\n",
      " 0.50630006 0.47587766 0.56356623 0.49082273 0.51945912 0.56322692\n",
      " 0.49231748 0.45420358 0.5018657  0.51083371]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  62%|███████████████████████████████████████████▏                          | 92/149 [29:16<18:05, 19.04s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  64%|█████████████████████████████████████████████                         | 96/149 [30:32<16:44, 18.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cluster centers:\n",
      "[[0.59069821 0.74168728 0.62103067 0.47193882 0.68446435 0.41047975\n",
      "  0.47397019 0.37905344 0.45925417 0.46993349]\n",
      " [0.37881737 0.35149037 0.39314329 0.48883852 0.4612476  0.56118882\n",
      "  0.51938671 0.56697491 0.49745146 0.49265705]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  68%|██████████████████████████████████████████████▊                      | 101/149 [32:06<14:59, 18.75s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  72%|█████████████████████████████████████████████████▌                   | 107/149 [33:58<13:02, 18.62s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  78%|█████████████████████████████████████████████████████▋               | 116/149 [36:46<10:20, 18.79s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  82%|████████████████████████████████████████████████████████▍            | 122/149 [38:39<08:26, 18.75s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  89%|█████████████████████████████████████████████████████████████▏       | 132/149 [41:45<05:17, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cluster centers:\n",
      "[[0.52881511 0.47159157 0.72018864 0.57138906 0.52921789 0.56740954\n",
      "  0.36703902 0.45377664 0.54807298 0.46216845]\n",
      " [0.5035153  0.51140753 0.26158121 0.41995083 0.42757688 0.51828902\n",
      "  0.63345135 0.58448202 0.43398429 0.57890726]]\n",
      "Mean cluster labels:\n",
      "[0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0\n",
      " 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  92%|███████████████████████████████████████████████████████████████▍     | 137/149 [43:22<03:51, 19.30s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  94%|████████████████████████████████████████████████████████████████▊    | 140/149 [44:18<02:51, 19.04s/it]/home/rajatthakur/.conda/envs/sci-code/lib/python3.10/site-packages/sklearn/base.py:1363: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Testing:  99%|████████████████████████████████████████████████████████████████████▌| 148/149 [46:49<00:18, 18.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.2327837249114142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████| 149/149 [47:08<00:00, 18.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Optimization Results\n",
      "================================================================================\n",
      " Technique Latency (ms) Throughput (samples/s) Memory (MB) Inference Success (%) Valid Code (%) Execution Success (%) Quality Score\n",
      "Base Model     10057.68                   0.10      4208.8                 100.0           43.6                  56.9          65.6\n",
      "   Pruning      5402.01                   0.19      4209.3                 100.0           39.6                  47.5          62.0\n",
      "\n",
      "Results saved to optimization_results_20250708_183526/optimization_results_20250708_183526.csv\n",
      "Visualization saved to optimization_results_20250708_183526/optimization_results_20250708_183526.png\n",
      "Detailed metrics saved to optimization_results_20250708_183526/detailed_metrics_20250708_183526.json\n",
      "\n",
      "Test pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch.nn.utils.prune as prune\n",
    "import sklearn\n",
    "import datetime\n",
    "\n",
    "CPU_MODE = False\n",
    "TEST_SAMPLE_SIZE = 100\n",
    "OPTIMIZATION_TECHNIQUES = [\"Base Model\", \"Pruning\"]\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = f\"optimization_results_{timestamp}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "print(f\"All results will be saved in: {results_dir}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./final-model\")\n",
    "\n",
    "def load_test_datasets():\n",
    "    test_prompts = []\n",
    "    try:\n",
    "        scidocs_path = \"scidocs_data\"\n",
    "        os.makedirs(scidocs_path, exist_ok=True)\n",
    "        if not os.path.exists(os.path.join(scidocs_path, \"paper_metadata_view_cite_read.json\")):\n",
    "            subprocess.run([\n",
    "                \"aws\", \"s3\", \"sync\", \"--no-sign-request\",\n",
    "                \"s3://ai2-s2-research-public/specter/scidocs/\",\n",
    "                scidocs_path, \"--region\", \"us-west-2\", \"--quiet\"\n",
    "            ], check=True)\n",
    "        with open(os.path.join(scidocs_path, \"paper_metadata_view_cite_read.json\"), \"r\") as f:\n",
    "            scidocs_data = json.load(f)\n",
    "        for i, (paper_id, content) in enumerate(scidocs_data.items()):\n",
    "            if i >= 30:\n",
    "                break\n",
    "            title = content.get('title', '') or ''\n",
    "            abstract = content.get('abstract', '') or ''\n",
    "            if len(title) > 10 and len(abstract) > 200:\n",
    "                test_prompts.append({\n",
    "                    \"text\": (\n",
    "                        f\"Generate a complete, self-contained Python code for text classification. \"\n",
    "                        f\"Title: {title}\\n\"\n",
    "                        f\"Abstract: {abstract[:400]}\\n\"\n",
    "                        \"Create a synthetic dataset based on the abstract and implement a classification model.\"\n",
    "                    ),\n",
    "                    \"source\": \"scidocs\",\n",
    "                    \"type\": \"classification\"\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"SciDocs loading failed: {str(e)}\")\n",
    "    try:\n",
    "        astronomy = load_dataset(\"David-Xu/astronomy-stack-dpo-text\", split=\"train\")\n",
    "        for i, example in enumerate(astronomy):\n",
    "            if i >= 30:\n",
    "                break\n",
    "            test_prompts.append({\n",
    "                \"text\": (\n",
    "                    \"Generate a complete, self-contained Python code to solve this astronomy problem:\\n\"\n",
    "                    f\"{example['prompt']}\\n\"\n",
    "                    \"Create any necessary synthetic data and implement a solution.\"\n",
    "                ),\n",
    "                \"source\": \"astronomy\",\n",
    "                \"type\": \"problem_solving\"\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Astronomy dataset loading failed: {str(e)}\")\n",
    "    try:\n",
    "        science = load_dataset(\"millawell/wikipedia_field_of_science\", split=\"train\")\n",
    "        for i, example in enumerate(science):\n",
    "            if i >= 30:\n",
    "                break\n",
    "            test_prompts.append({\n",
    "                \"text\": (\n",
    "                    \"Generate a complete, self-contained Python code for scientific text classification:\\n\"\n",
    "                    f\"Text: {example['text']}\\n\"\n",
    "                    \"Create a synthetic dataset and implement a classification model.\"\n",
    "                ),\n",
    "                \"source\": \"wikipedia_science\",\n",
    "                \"type\": \"classification\"\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Science dataset loading failed: {str(e)}\")\n",
    "    for i in range(20):\n",
    "        X, y = make_classification(\n",
    "            n_samples=100, \n",
    "            n_features=4, \n",
    "            n_informative=2, \n",
    "            n_classes=2,\n",
    "            random_state=i\n",
    "        )\n",
    "        data = pd.DataFrame(X, columns=[f\"feature_{j}\" for j in range(4)])\n",
    "        data[\"target\"] = y\n",
    "        test_prompts.append({\n",
    "            \"text\": \"Create a RandomForest classifier and show accuracy\",\n",
    "            \"data\": data,\n",
    "            \"source\": \"synthetic\",\n",
    "            \"type\": \"classification\"\n",
    "        })\n",
    "        X, y = make_blobs(n_samples=100, centers=3, cluster_std=1.5, random_state=i)\n",
    "        data = pd.DataFrame(X, columns=[\"x\", \"y\"])\n",
    "        test_prompts.append({\n",
    "            \"text\": \"Perform K-means clustering on this data\",\n",
    "            \"data\": data,\n",
    "            \"source\": \"synthetic\",\n",
    "            \"type\": \"clustering\"\n",
    "        })\n",
    "        X, _ = make_blobs(n_samples=100, centers=3, cluster_std=1.5, random_state=i)\n",
    "        outliers = np.random.uniform(low=-10, high=10, size=(5, 2))\n",
    "        X = np.vstack([X, outliers])\n",
    "        data = pd.DataFrame(X, columns=[\"x\", \"y\"])\n",
    "        test_prompts.append({\n",
    "            \"text\": \"Detect anomalies using Isolation Forest\",\n",
    "            \"data\": data,\n",
    "            \"source\": \"synthetic\",\n",
    "            \"type\": \"outlier_detection\"\n",
    "        })\n",
    "    print(f\"Created {len(test_prompts)} test prompts\")\n",
    "    return test_prompts\n",
    "\n",
    "def apply_optimization(technique_name):\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"./final-model\")\n",
    "        if technique_name == \"Pruning\":\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear) and \"lora\" not in name.lower():\n",
    "                    try:\n",
    "                        prune.l1_unstructured(module, name='weight', amount=0.1)\n",
    "                        prune.remove(module, 'weight')\n",
    "                    except:\n",
    "                        continue\n",
    "            return model\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying {technique_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def generate_robust_code(generator, prompt_text, task_type):\n",
    "    if task_type == \"classification\":\n",
    "        task_instructions = \"Focus on classification using RandomForestClassifier. Create synthetic data if needed.\"\n",
    "    elif task_type == \"clustering\":\n",
    "        task_instructions = \"Use KMeans clustering and visualize results with matplotlib.\"\n",
    "    elif task_type == \"outlier_detection\":\n",
    "        task_instructions = \"Use IsolationForest for outlier detection. Highlight anomalies in visualization.\"\n",
    "    elif task_type == \"problem_solving\":\n",
    "        task_instructions = \"Solve the problem using appropriate scientific computing techniques.\"\n",
    "    else:\n",
    "        task_instructions = \"Solve the problem efficiently with appropriate algorithms.\"\n",
    "    structured_prompt = f\"\"\"\n",
    "Generate complete, self-contained Python code to solve this task:\n",
    "{prompt_text}\n",
    "\n",
    "Specific Requirements:\n",
    "1. Create any necessary synthetic data if not provided\n",
    "2. Use only numpy, pandas, sklearn and matplotlib\n",
    "3. {task_instructions}\n",
    "4. Create complete, runnable code\n",
    "5. Print results clearly\n",
    "6. For visualizations, use plt.savefig('output.png') instead of plt.show()\n",
    "7. Ensure the code is syntactically correct\n",
    "\n",
    "Code:\n",
    "```python\n",
    "\"\"\"\n",
    "    try:\n",
    "        output = generator(\n",
    "            structured_prompt,\n",
    "            temperature=0.1,\n",
    "            max_new_tokens=700,\n",
    "            truncation=True,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        return output[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def validate_code(generated_code):\n",
    "    if not generated_code:\n",
    "        return \"\"\n",
    "    if \"```python\" in generated_code:\n",
    "        generated_code = generated_code.split(\"```python\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in generated_code:\n",
    "        generated_code = generated_code.split(\"```\")[1].split(\"```\")[0]\n",
    "    repairs = [\n",
    "        (r\"from sklearn\\.\\w+ import \\*\", \"\"),\n",
    "        (r\"fit\\(\\)\", \"fit(X_train, y_train)\"),\n",
    "        (r\"predict\\(\\)\", \"predict(X_test)\"),\n",
    "        (r\"plt\\.show\\(\\)\", \"plt.savefig('output.png')\"),\n",
    "        (r\"import matplotlib\\.pyplot as plt\", \"import matplotlib.pyplot as plt\\nplt.switch_backend('Agg')\"),\n",
    "        (r\"\\.to_csv\\('data\\.csv'\\)\", \"\")\n",
    "    ]\n",
    "    for pattern, replacement in repairs:\n",
    "        generated_code = re.sub(pattern, replacement, generated_code)\n",
    "    required_imports = [\n",
    "        \"import numpy as np\",\n",
    "        \"import pandas as pd\",\n",
    "        \"import matplotlib.pyplot as plt\"\n",
    "    ]\n",
    "    for imp in required_imports:\n",
    "        if imp not in generated_code:\n",
    "            generated_code = imp + \"\\n\" + generated_code\n",
    "    if \"from sklearn\" not in generated_code and \"import sklearn\" not in generated_code:\n",
    "        generated_code = \"from sklearn.ensemble import RandomForestClassifier, IsolationForest\\n\" + \\\n",
    "                         \"from sklearn.cluster import KMeans\\n\" + \\\n",
    "                         \"from sklearn.model_selection import train_test_split\\n\" + \\\n",
    "                         \"from sklearn.metrics import accuracy_score, classification_report\\n\" + generated_code\n",
    "    if \"pd.DataFrame\" not in generated_code and \"X =\" not in generated_code:\n",
    "        synthetic_data = \"\\nX = np.random.rand(100, 4)\\ny = np.random.randint(0, 2, 100)\\n\"\n",
    "        generated_code = generated_code.replace(\"import numpy as np\", \"import numpy as np\" + synthetic_data, 1)\n",
    "    return generated_code.strip()\n",
    "\n",
    "\n",
    "def safe_execute(code: str, data=None):\n",
    "    if not code:\n",
    "        return {\"status\": \"error\", \"message\": \"Empty code\"}\n",
    "    safe_env = {\n",
    "        \"__builtins__\": {\n",
    "            'print': print, 'range': range, 'len': len, 'str': str, 'int': int,\n",
    "            'float': float, 'bool': bool, 'list': list, 'dict': dict, 'tuple': tuple,\n",
    "            'set': set, 'min': min, 'max': max, 'sum': sum, 'abs': abs, 'round': round,\n",
    "            'enumerate': enumerate, 'zip': zip, '__import__': __import__\n",
    "        },\n",
    "        \"np\": np,\n",
    "        \"pd\": pd,\n",
    "        \"plt\": plt,\n",
    "        \"sklearn\": sklearn,\n",
    "        \"RandomForestClassifier\": RandomForestClassifier,\n",
    "        \"IsolationForest\": IsolationForest,\n",
    "        \"KMeans\": KMeans,\n",
    "        \"train_test_split\": train_test_split,\n",
    "        \"accuracy_score\": accuracy_score,\n",
    "        \"classification_report\": classification_report,\n",
    "    }\n",
    "    if data is not None:\n",
    "        safe_env[\"data\"] = data\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        exec(code, safe_env)\n",
    "        return {\"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"{type(e).__name__}: {str(e)}\"}\n",
    "\n",
    "def measure_inference_performance(generator, prompt_text, num_runs=3):\n",
    "    metrics = {\n",
    "        \"avg_latency\": 0,\n",
    "        \"throughput\": 0,\n",
    "        \"memory_usage\": 0,\n",
    "        \"success_rate\": 0\n",
    "    }\n",
    "    successes = 0\n",
    "    latencies = []\n",
    "    try:\n",
    "        _ = generator(prompt_text, max_new_tokens=50, truncation=True)\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            try:\n",
    "                run_start = time.time()\n",
    "                output = generator(\n",
    "                    prompt_text,\n",
    "                    max_new_tokens=300,\n",
    "                    truncation=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                latencies.append(time.time() - run_start)\n",
    "                successes += 1\n",
    "            except Exception:\n",
    "                continue\n",
    "        metrics[\"avg_latency\"] = np.mean(latencies) * 1000 if latencies else 0\n",
    "        metrics[\"throughput\"] = successes / max(0.001, time.time() - start_time)\n",
    "        metrics[\"success_rate\"] = successes / num_runs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            metrics[\"memory_usage\"] = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "        else:\n",
    "            metrics[\"memory_usage\"] = 0\n",
    "    except Exception as e:\n",
    "        print(f\"Performance measurement failed: {str(e)}\")\n",
    "    return metrics\n",
    "\n",
    "def evaluate_code_quality(generated_code):\n",
    "    if not generated_code:\n",
    "        return {\n",
    "            \"has_imports\": False,\n",
    "            \"has_model\": False,\n",
    "            \"has_print\": False,\n",
    "            \"is_runnable\": False,\n",
    "            \"score\": 0\n",
    "        }\n",
    "    has_imports = any(keyword in generated_code for keyword in \n",
    "                      [\"import numpy\", \"import pandas\", \"import sklearn\"])\n",
    "    has_model = any(keyword in generated_code for keyword in \n",
    "                    [\"RandomForest\", \"IsolationForest\", \"KMeans\"])\n",
    "    has_print = \"print(\" in generated_code\n",
    "    has_visualization = \"plt.savefig\" in generated_code or \"plt.plot\" in generated_code\n",
    "    has_data = \"X =\" in generated_code or \"pd.DataFrame\" in generated_code\n",
    "    is_runnable = has_imports and has_model and has_print and has_data\n",
    "    score = sum([has_imports, has_model, has_print, is_runnable, has_visualization]) / 5\n",
    "    return {\n",
    "        \"has_imports\": has_imports,\n",
    "        \"has_model\": has_model,\n",
    "        \"has_print\": has_print,\n",
    "        \"has_visualization\": has_visualization,\n",
    "        \"has_data\": has_data,\n",
    "        \"is_runnable\": is_runnable,\n",
    "        \"score\": score\n",
    "    }\n",
    "\n",
    "def run_test_pipeline():\n",
    "    test_prompts = load_test_datasets()\n",
    "    results = {}\n",
    "    for technique in OPTIMIZATION_TECHNIQUES:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Testing: {technique}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        model = apply_optimization(technique)\n",
    "        if model is None:\n",
    "            print(f\"Skipping {technique} due to initialization error\")\n",
    "            continue\n",
    "        model.eval()\n",
    "        device = 0 if torch.cuda.is_available() and not CPU_MODE else -1\n",
    "        print(f\"Using device: {'GPU' if device >= 0 else 'CPU'}\")\n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device\n",
    "        )\n",
    "        tech_results = {\n",
    "            \"inference\": {\n",
    "                \"avg_latency\": 0,\n",
    "                \"throughput\": 0,\n",
    "                \"memory_usage\": 0,\n",
    "                \"success_rate\": 0\n",
    "            },\n",
    "            \"quality\": {\n",
    "                \"syntax_errors\": 0,\n",
    "                \"execution_errors\": 0,\n",
    "                \"valid_count\": 0,\n",
    "                \"quality_score\": 0,\n",
    "                \"scores\": []\n",
    "            }\n",
    "        }\n",
    "        if test_prompts:\n",
    "            perf_metrics = measure_inference_performance(generator, test_prompts[0][\"text\"])\n",
    "            tech_results[\"inference\"] = perf_metrics\n",
    "        for item in tqdm(test_prompts, desc=\"Testing\"):\n",
    "            try:\n",
    "                generated = generate_robust_code(generator, item[\"text\"], item.get(\"type\", \"\"))\n",
    "                code = validate_code(generated)\n",
    "                if not code:\n",
    "                    tech_results[\"quality\"][\"syntax_errors\"] += 1\n",
    "                    tech_results[\"quality\"][\"scores\"].append(0)\n",
    "                    continue\n",
    "                quality_metrics = evaluate_code_quality(code)\n",
    "                quality_score = quality_metrics[\"score\"]\n",
    "                tech_results[\"quality\"][\"scores\"].append(quality_score)\n",
    "                if not quality_metrics[\"is_runnable\"]:\n",
    "                    tech_results[\"quality\"][\"syntax_errors\"] += 1\n",
    "                    continue\n",
    "                tech_results[\"quality\"][\"valid_count\"] += 1\n",
    "                data = item.get(\"data\", None)\n",
    "                if data is not None:\n",
    "                    exec_result = safe_execute(code, data)\n",
    "                    if exec_result[\"status\"] == \"error\":\n",
    "                        tech_results[\"quality\"][\"execution_errors\"] += 1\n",
    "            except Exception as e:\n",
    "                tech_results[\"quality\"][\"syntax_errors\"] += 1\n",
    "                tech_results[\"quality\"][\"scores\"].append(0)\n",
    "                print(f\"Error during testing: {str(e)}\")\n",
    "        if tech_results[\"quality\"][\"scores\"]:\n",
    "            tech_results[\"quality\"][\"quality_score\"] = np.mean(tech_results[\"quality\"][\"scores\"])\n",
    "        else:\n",
    "            tech_results[\"quality\"][\"quality_score\"] = 0\n",
    "        results[technique] = tech_results\n",
    "    return results\n",
    "\n",
    "def present_results(results):\n",
    "    table_data = []\n",
    "    for tech, metrics in results.items():\n",
    "        inf = metrics[\"inference\"]\n",
    "        qual = metrics[\"quality\"]\n",
    "        valid_count = qual[\"valid_count\"]\n",
    "        total_tests = len(qual[\"scores\"]) if \"scores\" in qual else TEST_SAMPLE_SIZE\n",
    "        table_data.append({\n",
    "            \"Technique\": tech,\n",
    "            \"Latency (ms)\": f\"{inf['avg_latency']:.2f}\",\n",
    "            \"Throughput (samples/s)\": f\"{inf['throughput']:.2f}\",\n",
    "            \"Memory (MB)\": f\"{inf['memory_usage']:.1f}\",\n",
    "            \"Inference Success (%)\": f\"{inf['success_rate'] * 100:.1f}\",\n",
    "            \"Valid Code (%)\": f\"{valid_count / total_tests * 100:.1f}\" if total_tests > 0 else \"N/A\",\n",
    "            \"Execution Success (%)\": f\"{(1 - qual['execution_errors'] / max(1, valid_count)) * 100:.1f}\" if valid_count > 0 else \"N/A\",\n",
    "            \"Quality Score\": f\"{qual['quality_score'] * 100:.1f}\"\n",
    "        })\n",
    "    results_df = pd.DataFrame(table_data)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Optimization Results\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df.to_string(index=False))\n",
    "    results_filename = f\"optimization_results_{timestamp}.csv\"\n",
    "    results_path = os.path.join(results_dir, results_filename)\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"\\nResults saved to {results_path}\")\n",
    "    if not results_df.empty:\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        results_df[\"Latency Value\"] = results_df[\"Latency (ms)\"].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "        results_df[\"Valid Code Value\"] = results_df[\"Valid Code (%)\"].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "        results_df[\"Exec Success Value\"] = results_df[\"Execution Success (%)\"].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "        results_df[\"Quality Value\"] = results_df[\"Quality Score\"].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "        results_df.plot.bar(x=\"Technique\", y=\"Latency Value\", ax=ax[0, 0], legend=False, color='skyblue')\n",
    "        ax[0, 0].set_title('Inference Latency')\n",
    "        ax[0, 0].set_ylabel('Milliseconds')\n",
    "        results_df.plot.bar(x=\"Technique\", y=\"Valid Code Value\", ax=ax[0, 1], legend=False, color='lightgreen')\n",
    "        ax[0, 1].set_title('Valid Code Rate')\n",
    "        ax[0, 1].set_ylabel('Percentage')\n",
    "        results_df.plot.bar(x=\"Technique\", y=\"Exec Success Value\", ax=ax[1, 0], legend=False, color='salmon')\n",
    "        ax[1, 0].set_title('Execution Success Rate')\n",
    "        ax[1, 0].set_ylabel('Percentage')\n",
    "        results_df.plot.bar(x=\"Technique\", y=\"Quality Value\", ax=ax[1, 1], legend=False, color='purple')\n",
    "        ax[1, 1].set_title('Code Quality Score')\n",
    "        ax[1, 1].set_ylabel('Score (0-100)')\n",
    "        plt.tight_layout()\n",
    "        viz_filename = f\"optimization_results_{timestamp}.png\"\n",
    "        viz_path = os.path.join(results_dir, viz_filename)\n",
    "        plt.savefig(viz_path, dpi=150)\n",
    "        print(f\"Visualization saved to {viz_path}\")\n",
    "        plt.close()\n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting comprehensive optimization comparison with {TEST_SAMPLE_SIZE} test prompts\")\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    results = run_test_pipeline()\n",
    "    results_df = present_results(results)\n",
    "    metrics_filename = f\"detailed_metrics_{timestamp}.json\"\n",
    "    metrics_path = os.path.join(results_dir, metrics_filename)\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"Detailed metrics saved to {metrics_path}\")\n",
    "    print(\"\\nTest pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
