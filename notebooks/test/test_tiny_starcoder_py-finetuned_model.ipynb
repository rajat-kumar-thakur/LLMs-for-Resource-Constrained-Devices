{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9e21e-66d4-4808-a3eb-493c19c730f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results will be saved in: optimization_results_20250709_200321\n",
      "Starting optimization comparison with 50 test prompts\n",
      "Created 50 test prompts\n",
      "\n",
      "========================================\n",
      "Testing: Base Model\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Testing:   8%|█████▊                                                                  | 4/50 [00:21<04:09,  5.43s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Testing:  50%|███████████████████████████████████▌                                   | 25/50 [02:15<02:16,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  56%|███████████████████████████████████████▊                               | 28/50 [02:32<02:00,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  60%|██████████████████████████████████████████▌                            | 30/50 [02:43<01:49,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  74%|████████████████████████████████████████████████████▌                  | 37/50 [03:22<01:11,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  86%|█████████████████████████████████████████████████████████████          | 43/50 [03:55<00:38,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 50/50 [04:33<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing: Pruning\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 50/50 [02:54<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing: Weight Sharing\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  86%|█████████████████████████████████████████████████████████████          | 43/50 [04:15<00:38,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 50/50 [04:54<00:00,  5.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing: Early Exit\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  54%|██████████████████████████████████████▎                                | 27/50 [02:27<02:06,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  74%|████████████████████████████████████████████████████▌                  | 37/50 [03:23<01:14,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  78%|███████████████████████████████████████████████████████▍               | 39/50 [03:37<01:10,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 50/50 [04:41<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Optimization Results\n",
      "================================================================================\n",
      "     Technique Latency (ms) Throughput (samples/s) Memory (MB) Inference Success (%)  Syntax Errors  Execution Errors Execution Success (%) Quality Score\n",
      "    Base Model       2664.1                   0.34       731.5                 100.0              5                45                   0.0           0.0\n",
      "       Pruning       2737.2                   0.33      1389.7                 100.0              0                48                   0.0           0.0\n",
      "Weight Sharing       2711.6                   0.34      1389.9                 100.0              1                49                   0.0           0.0\n",
      "    Early Exit       2571.3                   0.36      1390.2                 100.0              3                47                   0.0           0.0\n",
      "\n",
      "Results saved to optimization_results_20250709_200321/optimization_results_20250709_200321.csv\n",
      "Visualization saved to optimization_results_20250709_200321/optimization_results_20250709_200321.png\n",
      "Detailed metrics saved to optimization_results_20250709_200321/detailed_metrics_20250709_200321.json\n",
      "\n",
      "Test pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch.nn.utils.prune as prune\n",
    "import sklearn\n",
    "import datetime\n",
    "import gc\n",
    "from peft import PeftModel\n",
    "\n",
    "CPU_MODE = False\n",
    "TEST_SAMPLE_SIZE = 50\n",
    "OPTIMIZATION_TECHNIQUES = [\n",
    "    \"Base Model\",\n",
    "    \"Pruning\",\n",
    "    \"Weight Sharing\",\n",
    "    \"Early Exit\"\n",
    "]\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = f\"optimization_results_{timestamp}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "print(f\"All results will be saved in: {results_dir}\")\n",
    "\n",
    "base_model_name = \"bigcode/tiny_starcoder_py\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=None\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, \"./codegen-lora-adapters\")\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"./final-model\")\n",
    "tokenizer.save_pretrained(\"./final-model\")\n",
    "\n",
    "def apply_optimization(technique_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"./final-model\", device_map=None)\n",
    "    if technique_name == \"Pruning\":\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear) and \"lora\" not in name.lower():\n",
    "                prune.ln_structured(module, name='weight', amount=0.15, n=2, dim=0)\n",
    "                prune.remove(module, 'weight')\n",
    "        return model\n",
    "    if technique_name == \"Weight Sharing\":\n",
    "        if hasattr(model, 'lm_head') and hasattr(model, 'model'):\n",
    "            if hasattr(model.model, 'embed_tokens'):\n",
    "                model.lm_head.weight = model.model.embed_tokens.weight\n",
    "        return model\n",
    "    return model\n",
    "\n",
    "def load_test_datasets():\n",
    "    test_prompts = []\n",
    "    try:\n",
    "        scidocs_path = \"scidocs_data\"\n",
    "        os.makedirs(scidocs_path, exist_ok=True)\n",
    "        if not os.path.exists(os.path.join(scidocs_path, \"paper_metadata_view_cite_read.json\")):\n",
    "            subprocess.run([\n",
    "                \"aws\", \"s3\", \"sync\", \"--no-sign-request\",\n",
    "                \"s3://ai2-s2-research-public/specter/scidocs/\",\n",
    "                scidocs_path, \"--region\", \"us-west-2\", \"--quiet\"\n",
    "            ], check=True)\n",
    "        with open(os.path.join(scidocs_path, \"paper_metadata_view_cite_read.json\"), \"r\") as f:\n",
    "            scidocs_data = json.load(f)\n",
    "        for i, (paper_id, content) in enumerate(scidocs_data.items()):\n",
    "            if i >= 10: break\n",
    "            title = content.get('title', '') or ''\n",
    "            abstract = content.get('abstract', '') or ''\n",
    "            if len(title) > 10 and len(abstract) > 200:\n",
    "                test_prompts.append({\n",
    "                    \"text\": (\n",
    "                        f\"Generate complete Python code for: {title}\\n\"\n",
    "                        f\"Abstract: {abstract[:300]}\\n\"\n",
    "                        \"Create a synthetic dataset and implement analysis.\"\n",
    "                    ),\n",
    "                    \"source\": \"scidocs\",\n",
    "                    \"type\": \"scientific\"\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"SciDocs loading failed: {str(e)}\")\n",
    "    try:\n",
    "        astronomy = load_dataset(\"David-Xu/astronomy-stack-dpo-text\", split=\"train\")\n",
    "        for i, example in enumerate(astronomy):\n",
    "            if i >= 10: break\n",
    "            test_prompts.append({\n",
    "                \"text\": (\n",
    "                    f\"Generate Python code to solve: {example['prompt']}\\n\"\n",
    "                    \"Create any necessary synthetic data.\"\n",
    "                ),\n",
    "                \"source\": \"astronomy\",\n",
    "                \"type\": \"problem_solving\"\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Astronomy dataset loading failed: {str(e)}\")\n",
    "    try:\n",
    "        science = load_dataset(\"millawell/wikipedia_field_of_science\", split=\"train\")\n",
    "        for i, example in enumerate(science):\n",
    "            if i >= 10: break\n",
    "            test_prompts.append({\n",
    "                \"text\": (\n",
    "                    f\"Generate classification code for: {example['text']}\\n\"\n",
    "                    \"Create a synthetic dataset and implement classification.\"\n",
    "                ),\n",
    "                \"source\": \"wikipedia_science\",\n",
    "                \"type\": \"classification\"\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Science dataset loading failed: {str(e)}\")\n",
    "    for i in range(10):\n",
    "        X, y = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=i)\n",
    "        data = pd.DataFrame(X, columns=[f\"feature_{j}\" for j in range(4)])\n",
    "        data[\"target\"] = y\n",
    "        test_prompts.append({\n",
    "            \"text\": (\n",
    "                \"Create a RandomForest classifier with train-test split and show accuracy\\n\"\n",
    "                \"The input data is in a DataFrame 'df' with features and 'target' column\\n\"\n",
    "                \"Steps:\\n\"\n",
    "                \"1. Split into features (X) and target (y)\\n\"\n",
    "                \"2. Create train/test splits\\n\"\n",
    "                \"3. Train classifier\\n\"\n",
    "                \"4. Make predictions\\n\"\n",
    "                \"5. Print accuracy\"\n",
    "            ),\n",
    "            \"data\": data,\n",
    "            \"source\": \"synthetic\",\n",
    "            \"type\": \"classification\"\n",
    "        })\n",
    "        X, y = make_blobs(n_samples=100, centers=3, cluster_std=1.5, random_state=i)\n",
    "        data = pd.DataFrame(X, columns=[\"x\", \"y\"])\n",
    "        test_prompts.append({\n",
    "            \"text\": (\n",
    "                \"Perform K-means clustering and visualize results\\n\"\n",
    "                \"The input data is in a DataFrame 'df' with columns 'x' and 'y'\\n\"\n",
    "                \"Steps:\\n\"\n",
    "                \"1. Prepare data\\n\"\n",
    "                \"2. Fit KMeans model\\n\"\n",
    "                \"3. Predict clusters\\n\"\n",
    "                \"4. Create scatter plot colored by cluster\\n\"\n",
    "                \"5. Save plot as 'clusters.png'\"\n",
    "            ),\n",
    "            \"data\": data,\n",
    "            \"source\": \"synthetic\",\n",
    "            \"type\": \"clustering\"\n",
    "        })\n",
    "    print(f\"Created {len(test_prompts)} test prompts\")\n",
    "    return test_prompts\n",
    "\n",
    "def generate_robust_code(generator, prompt_text, task_type):\n",
    "    task_instructions = {\n",
    "        \"scientific\": (\n",
    "            \"Implement complete scientific analysis using numpy/pandas\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"1. Create synthetic dataset\\n\"\n",
    "            \"2. Perform meaningful calculations\\n\"\n",
    "            \"3. Print clear results\\n\"\n",
    "            \"4. DO NOT just import libraries without using them\"\n",
    "        ),\n",
    "        \"problem_solving\": (\n",
    "            \"Solve with scientific computing techniques\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"1. Define the problem\\n\"\n",
    "            \"2. Implement complete solution\\n\"\n",
    "            \"3. Print the final answer\\n\"\n",
    "            \"4. DO NOT just import libraries without using them\"\n",
    "        ),\n",
    "        \"classification\": (\n",
    "            \"Use RandomForestClassifier with train-test split\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"1. Split data into features (X) and target (y)\\n\"\n",
    "            \"2. Create train/test splits\\n\"\n",
    "            \"3. Train classifier\\n\"\n",
    "            \"4. Make predictions\\n\"\n",
    "            \"5. Print accuracy\\n\"\n",
    "            \"6. DO NOT just import libraries without using them\"\n",
    "        ),\n",
    "        \"clustering\": (\n",
    "            \"Use KMeans clustering and visualize results\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"1. Prepare data\\n\"\n",
    "            \"2. Fit KMeans model\\n\"\n",
    "            \"3. Predict clusters\\n\"\n",
    "            \"4. Create scatter plot colored by cluster\\n\"\n",
    "            \"5. Save plot as 'clusters.png'\\n\"\n",
    "            \"6. DO NOT just import libraries without using them\"\n",
    "        )\n",
    "    }.get(task_type, \"Implement complete solution with meaningful operations\")\n",
    "    \n",
    "    structured_prompt = f\"\"\"\n",
    "Generate complete, self-contained Python code to solve:\n",
    "{prompt_text}\n",
    "\n",
    "Specific Instructions:\n",
    "{task_instructions}\n",
    "\n",
    "Code must:\n",
    "- Use ONLY numpy, pandas, sklearn, matplotlib\n",
    "- Print results clearly\n",
    "- For visualizations: plt.savefig('output.png')\n",
    "- Have NO unused imports\n",
    "- Be syntactically correct\n",
    "\n",
    "Code:\n",
    "```python\n",
    "\"\"\"\n",
    "    try:\n",
    "        output = generator(\n",
    "            structured_prompt,\n",
    "            temperature=0.1,\n",
    "            max_new_tokens=512,\n",
    "            truncation=True,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        return output[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def validate_code(generated_code):\n",
    "    if not generated_code:\n",
    "        return \"\", {\"numpy\": False, \"pandas\": False, \"sklearn\": False, \"matplotlib\": False}\n",
    "    \n",
    "    if \"```python\" in generated_code:\n",
    "        code = generated_code.split(\"```python\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in generated_code:\n",
    "        code = generated_code.split(\"```\")[1].split(\"```\")[0]\n",
    "    else:\n",
    "        code = generated_code\n",
    "    \n",
    "    lib_usage = {\n",
    "        \"numpy\": False,\n",
    "        \"pandas\": False,\n",
    "        \"sklearn\": False,\n",
    "        \"matplotlib\": False\n",
    "    }\n",
    "    \n",
    "    repairs = [\n",
    "        (r\"from\\s+sklearn\\s+import\\s+\\*\", \n",
    "         \"from sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.cluster import KMeans\"),\n",
    "        (r\"classifier\\.fit\\(\\)\", \"classifier.fit(X_train, y_train)\"),\n",
    "        (r\"predict\\(\\)\", \"predict(X_test)\"),\n",
    "        (r\"plt\\.show\\(\\)\", \"plt.savefig('output.png')\"),\n",
    "        (r\"import matplotlib\\.pyplot as plt\", \n",
    "         \"import matplotlib.pyplot as plt\\nplt.switch_backend('Agg')\"),\n",
    "        (r\"\\.to_csv\\('data\\.csv'\\)\", \"\")\n",
    "    ]\n",
    "    \n",
    "    for pattern, replacement in repairs:\n",
    "        code = re.sub(pattern, replacement, code)\n",
    "    \n",
    "    if \"np.\" in code or \"numpy.\" in code:\n",
    "        lib_usage[\"numpy\"] = True\n",
    "    if \"pd.\" in code or \"pandas.\" in code:\n",
    "        lib_usage[\"pandas\"] = True\n",
    "    if \"train_test_split\" in code or \"RandomForestClassifier\" in code or \"KMeans\" in code:\n",
    "        lib_usage[\"sklearn\"] = True\n",
    "    if \"plt.\" in code or \"matplotlib.\" in code:\n",
    "        lib_usage[\"matplotlib\"] = True\n",
    "    \n",
    "    required_imports = []\n",
    "    if lib_usage[\"numpy\"]:\n",
    "        required_imports.append(\"import numpy as np\")\n",
    "    if lib_usage[\"pandas\"]:\n",
    "        required_imports.append(\"import pandas as pd\")\n",
    "    if lib_usage[\"matplotlib\"]:\n",
    "        required_imports.append(\"import matplotlib.pyplot as plt\")\n",
    "    if lib_usage[\"sklearn\"]:\n",
    "        required_imports.append(\"from sklearn.ensemble import RandomForestClassifier\")\n",
    "        required_imports.append(\"from sklearn.cluster import KMeans\")\n",
    "        required_imports.append(\"from sklearn.model_selection import train_test_split\")\n",
    "        required_imports.append(\"from sklearn.metrics import accuracy_score\")\n",
    "    \n",
    "    for imp in required_imports:\n",
    "        if imp not in code:\n",
    "            code = imp + \"\\n\" + code\n",
    "    \n",
    "    code_lines = code.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in code_lines:\n",
    "        if line.strip().startswith(\"import\") or line.strip().startswith(\"from\"):\n",
    "            lib_name = line.split()[1].split(\".\")[0] if \"import\" in line else line.split()[1]\n",
    "            if lib_name in [\"numpy\", \"np\", \"pandas\", \"pd\", \"sklearn\", \"matplotlib\", \"plt\"]:\n",
    "                if any(f\"{lib_name}.\" in line for line in code_lines):\n",
    "                    cleaned_lines.append(line)\n",
    "        else:\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines), lib_usage\n",
    "\n",
    "def safe_execute(code: str, data=None):\n",
    "    if not code:\n",
    "        return {\"status\": \"error\", \"message\": \"Empty code\"}\n",
    "    \n",
    "    safe_env = {\n",
    "        \"__builtins__\": {\n",
    "            'print': print, 'range': range, 'len': len, 'str': str, 'int': int, \n",
    "            'float': float, 'bool': bool, 'list': list, 'dict': dict, 'tuple': tuple, \n",
    "            'set': set, 'min': min, 'max': max, 'sum': sum, 'abs': abs, 'round': round,\n",
    "            'enumerate': enumerate, 'zip': zip\n",
    "        },\n",
    "        \"np\": np,\n",
    "        \"pd\": pd,\n",
    "        \"plt\": plt,\n",
    "        \"RandomForestClassifier\": RandomForestClassifier,\n",
    "        \"KMeans\": KMeans,\n",
    "        \"train_test_split\": train_test_split,\n",
    "        \"accuracy_score\": accuracy_score,\n",
    "    }\n",
    "    \n",
    "    if data is not None:\n",
    "        safe_env[\"df\"] = data\n",
    "        \n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        exec(code, safe_env)\n",
    "        return {\"status\": \"success\", \"env\": safe_env}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"{type(e).__name__}: {str(e)}\"}\n",
    "\n",
    "def measure_inference_performance(generator, prompt_text, num_runs=5):\n",
    "    metrics = {\n",
    "        \"avg_latency\": 0,\n",
    "        \"throughput\": 0,\n",
    "        \"memory_usage\": 0,\n",
    "        \"success_rate\": 0\n",
    "    }\n",
    "    successes = 0\n",
    "    latencies = []\n",
    "    try:\n",
    "        _ = generator(prompt_text, max_new_tokens=50)\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            try:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                run_start = time.time()\n",
    "                output = generator(\n",
    "                    prompt_text,\n",
    "                    max_new_tokens=256,\n",
    "                    truncation=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                latencies.append(time.time() - run_start)\n",
    "                successes += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Inference error: {str(e)}\")\n",
    "                continue\n",
    "        metrics[\"avg_latency\"] = np.mean(latencies) * 1000 if latencies else 0\n",
    "        metrics[\"throughput\"] = successes / max(0.001, time.time() - start_time)\n",
    "        metrics[\"success_rate\"] = successes / num_runs\n",
    "        if torch.cuda.is_available():\n",
    "            metrics[\"memory_usage\"] = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "        else:\n",
    "            metrics[\"memory_usage\"] = 0\n",
    "    except Exception as e:\n",
    "        print(f\"Performance measurement failed: {str(e)}\")\n",
    "    return metrics\n",
    "\n",
    "def evaluate_execution_success(exec_result, task_type, lib_usage):\n",
    "    if exec_result[\"status\"] != \"success\":\n",
    "        return False\n",
    "    env = exec_result.get(\"env\", {})\n",
    "    if task_type == \"classification\" and not lib_usage.get(\"sklearn\", False):\n",
    "        return False\n",
    "    if task_type == \"clustering\" and not lib_usage.get(\"sklearn\", False):\n",
    "        return False\n",
    "    if task_type == \"scientific\" and not any(lib_usage.values()):\n",
    "        return False\n",
    "    if task_type == \"classification\":\n",
    "        return \"accuracy\" in env or \"Accuracy\" in str(env)\n",
    "    elif task_type == \"clustering\":\n",
    "        return \"KMeans\" in str(env) and \"clusters\" in str(env)\n",
    "    elif task_type == \"scientific\":\n",
    "        return \"results\" in str(env) or \"analysis\" in str(env)\n",
    "    elif task_type == \"problem_solving\":\n",
    "        return \"solution\" in str(env) or \"answer\" in str(env)\n",
    "    return \"print\" in str(env) and \"=\" in str(env)\n",
    "\n",
    "def run_test_pipeline():\n",
    "    test_prompts = load_test_datasets()\n",
    "    num_test_samples = len(test_prompts)\n",
    "    results = {}\n",
    "    for technique in OPTIMIZATION_TECHNIQUES:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Testing: {technique}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        model = apply_optimization(technique)\n",
    "        if model is None:\n",
    "            print(f\"Skipping {technique} due to error\")\n",
    "            continue\n",
    "        model.eval()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not CPU_MODE else \"cpu\")\n",
    "        model.to(device)\n",
    "        print(f\"Using device: {device}\")\n",
    "        generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
    "        tech_results = {\n",
    "            \"inference\": {\n",
    "                \"avg_latency\": 0,\n",
    "                \"throughput\": 0,\n",
    "                \"memory_usage\": 0,\n",
    "                \"success_rate\": 0\n",
    "            },\n",
    "            \"quality\": {\n",
    "                \"syntax_errors\": 0,\n",
    "                \"execution_errors\": 0,\n",
    "                \"execution_success\": 0,\n",
    "                \"valid_count\": 0,\n",
    "                \"quality_score\": 0\n",
    "            }\n",
    "        }\n",
    "        perf_metrics = measure_inference_performance(generator, test_prompts[0][\"text\"])\n",
    "        tech_results[\"inference\"] = perf_metrics\n",
    "        for item in tqdm(test_prompts, desc=\"Testing\"):\n",
    "            try:\n",
    "                generated = generate_robust_code(generator, item[\"text\"], item.get(\"type\", \"\"))\n",
    "                code, lib_usage = validate_code(generated)\n",
    "                if not code:\n",
    "                    tech_results[\"quality\"][\"syntax_errors\"] += 1\n",
    "                    continue\n",
    "                data = item.get(\"data\", None)\n",
    "                exec_result = safe_execute(code, data)\n",
    "                if exec_result[\"status\"] == \"error\":\n",
    "                    tech_results[\"quality\"][\"execution_errors\"] += 1\n",
    "                else:\n",
    "                    tech_results[\"quality\"][\"valid_count\"] += 1\n",
    "                    if evaluate_execution_success(exec_result, item.get(\"type\", \"\"), lib_usage):\n",
    "                        tech_results[\"quality\"][\"execution_success\"] += 1\n",
    "            except Exception as e:\n",
    "                tech_results[\"quality\"][\"syntax_errors\"] += 1\n",
    "                print(f\"Test error: {str(e)}\")\n",
    "        tech_results[\"quality\"][\"quality_score\"] = (tech_results[\"quality\"][\"execution_success\"] / max(1, num_test_samples))\n",
    "        results[technique] = tech_results\n",
    "    return results, num_test_samples\n",
    "\n",
    "def present_results(results, num_test_samples):\n",
    "    table_data = []\n",
    "    for tech, metrics in results.items():\n",
    "        inf = metrics[\"inference\"]\n",
    "        qual = metrics[\"quality\"]\n",
    "        table_data.append({\n",
    "            \"Technique\": tech,\n",
    "            \"Latency (ms)\": inf['avg_latency'],\n",
    "            \"Throughput (samples/s)\": inf['throughput'],\n",
    "            \"Memory (MB)\": inf['memory_usage'],\n",
    "            \"Inference Success (%)\": inf['success_rate'] * 100,\n",
    "            \"Syntax Errors\": qual[\"syntax_errors\"],\n",
    "            \"Execution Errors\": qual[\"execution_errors\"],\n",
    "            \"Execution Success (%)\": qual['execution_success'] / num_test_samples * 100,\n",
    "            \"Quality Score\": qual['quality_score'] * 100\n",
    "        })\n",
    "    results_df = pd.DataFrame(table_data)\n",
    "    formatted_df = results_df.copy()\n",
    "    formatted_df[\"Latency (ms)\"] = formatted_df[\"Latency (ms)\"].apply(lambda x: f\"{x:.1f}\")\n",
    "    formatted_df[\"Throughput (samples/s)\"] = formatted_df[\"Throughput (samples/s)\"].apply(lambda x: f\"{x:.2f}\")\n",
    "    formatted_df[\"Memory (MB)\"] = formatted_df[\"Memory (MB)\"].apply(lambda x: f\"{x:.1f}\")\n",
    "    formatted_df[\"Inference Success (%)\"] = formatted_df[\"Inference Success (%)\"].apply(lambda x: f\"{x:.1f}\")\n",
    "    formatted_df[\"Execution Success (%)\"] = formatted_df[\"Execution Success (%)\"].apply(lambda x: f\"{x:.1f}\")\n",
    "    formatted_df[\"Quality Score\"] = formatted_df[\"Quality Score\"].apply(lambda x: f\"{x:.1f}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Optimization Results\")\n",
    "    print(\"=\"*80)\n",
    "    print(formatted_df.to_string(index=False))\n",
    "    results_filename = f\"optimization_results_{timestamp}.csv\"\n",
    "    results_path = os.path.join(results_dir, results_filename)\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"\\nResults saved to {results_path}\")\n",
    "    if not results_df.empty:\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        results_df.plot.bar(x=\"Technique\", y=\"Latency (ms)\", ax=ax[0, 0], legend=False, color='skyblue')\n",
    "        ax[0, 0].set_title('Inference Latency')\n",
    "        ax[0, 0].set_ylabel('Milliseconds')\n",
    "        results_df.plot.bar(x=\"Technique\", y=\"Memory (MB)\", ax=ax[0, 1], legend=False, color='lightgreen')\n",
    "        ax[0, 1].set_title('Memory Usage')\n",
    "        ax[0, 1].set_ylabel('MB')\n",
    "        results_df.plot.bar(x=\"Technique\", y=\"Execution Success (%)\", ax=ax[1, 0], legend=False, color='salmon')\n",
    "        ax[1, 0].set_title('Execution Success Rate')\n",
    "        ax[1, 0].set_ylabel('Percentage')\n",
    "        results_df.plot.bar(x=\"Technique\", y=\"Quality Score\", ax=ax[1, 1], legend=False, color='purple')\n",
    "        ax[1, 1].set_title('Code Quality Score')\n",
    "        ax[1, 1].set_ylabel('Score')\n",
    "        plt.tight_layout()\n",
    "        viz_filename = f\"optimization_results_{timestamp}.png\"\n",
    "        viz_path = os.path.join(results_dir, viz_filename)\n",
    "        plt.savefig(viz_path, dpi=150)\n",
    "        print(f\"Visualization saved to {viz_path}\")\n",
    "        plt.close()\n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting optimization comparison with {TEST_SAMPLE_SIZE} test prompts\")\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    results, num_test_samples = run_test_pipeline()\n",
    "    results_df = present_results(results, num_test_samples)\n",
    "    metrics_filename = f\"detailed_metrics_{timestamp}.json\"\n",
    "    metrics_path = os.path.join(results_dir, metrics_filename)\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"Detailed metrics saved to {metrics_path}\")\n",
    "    print(\"\\nTest pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189b351-8a9e-4336-9ce9-b150e8e1ffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results will be saved in: optimization_results_20250709_221854\n",
      "Created 5 test prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing Base Model on 5 Prompts\n",
      "============================================================\n",
      "\n",
      "\n",
      "========================================\n",
      "PROMPT 1/5 [scidocs]\n",
      "Type: scientific\n",
      "Content:\n",
      "Generate complete Python code for: Pelvic Girdle Pain during or after Pregnancy: a review of recent evidence and a clinical care path proposal\n",
      "Abstract: PROBLEM STATEMENT\n",
      "Pelvic girdle pain (PGP) is a common condition during or after pregnancy with pain and disability as most important symptoms. These symptoms have a wide range of clinical presentation. Most doctors perceive pregnancy related pelvic girdle pain (PPGP) as 'physiologic' or 'expected d\n",
      "Create a synthetic dataset and implement analy...\n",
      "----------------------------------------\n",
      "\n",
      "GENERATED CODE:\n",
      "----------------------------------------\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.switch_backend('Agg')\n",
      "\n",
      "# Generate synthetic dataset\n",
      "data = np.random.randn(100, 10)\n",
      "data\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "EXECUTION ERROR: ImportError: __import__ not found\n",
      "\n",
      "Completed prompt 1/5\n",
      "\n",
      "\n",
      "========================================\n",
      "PROMPT 2/5 [scidocs]\n",
      "Type: scientific\n",
      "Content:\n",
      "Generate complete Python code for: Packet Classification Using Tuple Space Search\n",
      "Abstract: Routers must perform packet classification at high speeds to efficiently implement functions such as firewalls and QoS routing. Packet classification requires matching each packet against a database of filters (or rules), and forwarding the packet according to the highest priority filter. Existing f\n",
      "Create a synthetic dataset and implement analysis....\n",
      "----------------------------------------\n",
      "\n",
      "GENERATED CODE:\n",
      "----------------------------------------\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.switch_backend('Agg')\n",
      "\n",
      "dataset = pd.read_csv('data/dataset.csv')\n",
      "dataset.head()\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "EXECUTION ERROR: ImportError: __import__ not found\n",
      "\n",
      "Completed prompt 2/5\n",
      "\n",
      "\n",
      "========================================\n",
      "PROMPT 3/5 [synthetic]\n",
      "Type: classification\n",
      "Content:\n",
      "Create a RandomForest classifier with train-test split and show accuracy\n",
      "The input data is in a DataFrame 'df' with features and 'target' column\n",
      "Steps:\n",
      "1. Split into features (X) and target (y)\n",
      "2. Create train/test splits\n",
      "3. Train classifier\n",
      "4. Make predictions\n",
      "5. Print accuracy...\n",
      "----------------------------------------\n",
      "\n",
      "GENERATED CODE:\n",
      "----------------------------------------\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_validate\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.model_selection import cross_validate_score\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.model_selection import cross_validate_predict\n",
      "from sklearn.model_selection import cross_validate_score\n",
      "from sklearn.model_selection import cross_validate_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate_train_test_split\n",
      "from sklearn.model_selection import cross_validate\n",
      "----------------------------------------\n",
      "\n",
      "EXECUTION ERROR: ImportError: __import__ not found\n",
      "\n",
      "Completed prompt 3/5\n",
      "\n",
      "\n",
      "========================================\n",
      "PROMPT 4/5 [synthetic]\n",
      "Type: classification\n",
      "Content:\n",
      "Create a RandomForest classifier with train-test split and show accuracy\n",
      "The input data is in a DataFrame 'df' with features and 'target' column\n",
      "Steps:\n",
      "1. Split into features (X) and target (y)\n",
      "2. Create train/test splits\n",
      "3. Train classifier\n",
      "4. Make predictions\n",
      "5. Print accuracy...\n",
      "----------------------------------------\n",
      "\n",
      "GENERATED CODE:\n",
      "----------------------------------------\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict_proba\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict_log_proba\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn\n",
      "----------------------------------------\n",
      "\n",
      "EXECUTION ERROR: SyntaxError: invalid syntax (<unknown>, line 42)\n",
      "\n",
      "Completed prompt 4/5\n",
      "\n",
      "\n",
      "========================================\n",
      "PROMPT 5/5 [synthetic]\n",
      "Type: classification\n",
      "Content:\n",
      "Create a RandomForest classifier with train-test split and show accuracy\n",
      "The input data is in a DataFrame 'df' with features and 'target' column\n",
      "Steps:\n",
      "1. Split into features (X) and target (y)\n",
      "2. Create train/test splits\n",
      "3. Train classifier\n",
      "4. Make predictions\n",
      "5. Print accuracy...\n",
      "----------------------------------------\n",
      "\n",
      "GENERATED CODE:\n",
      "----------------------------------------\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import Strat\n",
      "----------------------------------------\n",
      "\n",
      "EXECUTION ERROR: ImportError: __import__ not found\n",
      "\n",
      "Completed prompt 5/5\n",
      "\n",
      "============================================================\n",
      "Test completed! Results saved to optimization_results_20250709_221854/base_model_results_20250709_221854.json\n",
      "============================================================\n",
      "\n",
      "EXECUTION SUMMARY:\n",
      "------------------------------------------------------------\n",
      "Successful executions: 0/5\n",
      "Prompt 1: scidocs (scientific) -> ERROR: ImportError: __import__ not found\n",
      "Prompt 2: scidocs (scientific) -> ERROR: ImportError: __import__ not found\n",
      "Prompt 3: synthetic (classification) -> ERROR: ImportError: __import__ not found\n",
      "Prompt 4: synthetic (classification) -> ERROR: SyntaxError: invalid syntax (<unknown>, line 42)\n",
      "Prompt 5: synthetic (classification) -> ERROR: ImportError: __import__ not found\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch.nn.utils.prune as prune\n",
    "import sklearn\n",
    "import datetime\n",
    "import gc\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "CPU_MODE = False\n",
    "TEST_SAMPLE_SIZE = 5\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = f\"optimization_results_{timestamp}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "print(f\"All results will be saved in: {results_dir}\")\n",
    "\n",
    "base_model_name = \"bigcode/tiny_starcoder_py\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=None\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, \"./codegen-lora-adapters\")\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"./final-model\")\n",
    "tokenizer.save_pretrained(\"./final-model\")\n",
    "\n",
    "def load_test_datasets():\n",
    "    test_prompts = []\n",
    "    try:\n",
    "        scidocs_path = \"scidocs_data\"\n",
    "        os.makedirs(scidocs_path, exist_ok=True)\n",
    "        if not os.path.exists(os.path.join(scidocs_path, \"paper_metadata_view_cite_read.json\")):\n",
    "            subprocess.run([\n",
    "                \"aws\", \"s3\", \"sync\", \"--no-sign-request\",\n",
    "                \"s3://ai2-s2-research-public/specter/scidocs/\",\n",
    "                scidocs_path, \"--region\", \"us-west-2\", \"--quiet\"\n",
    "            ], check=True)\n",
    "        with open(os.path.join(scidocs_path, \"paper_metadata_view_cite_read.json\"), \"r\") as f:\n",
    "            scidocs_data = json.load(f)\n",
    "        for i, (paper_id, content) in enumerate(scidocs_data.items()):\n",
    "            if i >= 2: break\n",
    "            title = content.get('title', '') or ''\n",
    "            abstract = content.get('abstract', '') or ''\n",
    "            if len(title) > 10 and len(abstract) > 200:\n",
    "                test_prompts.append({\n",
    "                    \"text\": (\n",
    "                        f\"Generate complete Python code for: {title}\\n\"\n",
    "                        f\"Abstract: {abstract[:300]}\\n\"\n",
    "                        \"Create a synthetic dataset and implement analysis.\"\n",
    "                    ),\n",
    "                    \"source\": \"scidocs\",\n",
    "                    \"type\": \"scientific\"\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"SciDocs loading failed: {str(e)}\")\n",
    "    for i in range(3):\n",
    "        X, y = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=i)\n",
    "        data = pd.DataFrame(X, columns=[f\"feature_{j}\" for j in range(4)])\n",
    "        data[\"target\"] = y\n",
    "        test_prompts.append({\n",
    "            \"text\": (\n",
    "                \"Create a RandomForest classifier with train-test split and show accuracy\\n\"\n",
    "                \"The input data is in a DataFrame 'df' with features and 'target' column\\n\"\n",
    "                \"Steps:\\n\"\n",
    "                \"1. Split into features (X) and target (y)\\n\"\n",
    "                \"2. Create train/test splits\\n\"\n",
    "                \"3. Train classifier\\n\"\n",
    "                \"4. Make predictions\\n\"\n",
    "                \"5. Print accuracy\"\n",
    "            ),\n",
    "            \"data\": data,\n",
    "            \"source\": \"synthetic\",\n",
    "            \"type\": \"classification\"\n",
    "        })\n",
    "    print(f\"Created {len(test_prompts)} test prompts\")\n",
    "    return test_prompts\n",
    "\n",
    "def generate_robust_code(generator, prompt_text, task_type):\n",
    "    task_instructions = {\n",
    "        \"scientific\": (\n",
    "            \"Implement complete scientific analysis using numpy/pandas\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"1. Create synthetic dataset\\n\"\n",
    "            \"2. Perform meaningful calculations\\n\"\n",
    "            \"3. Print clear results\\n\"\n",
    "            \"4. DO NOT just import libraries without using them\"\n",
    "        ),\n",
    "        \"classification\": (\n",
    "            \"Use RandomForestClassifier with train-test split\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"1. Split data into features (X) and target (y)\\n\"\n",
    "            \"2. Create train/test splits\\n\"\n",
    "            \"3. Train classifier\\n\"\n",
    "            \"4. Make predictions\\n\"\n",
    "            \"5. Print accuracy\\n\"\n",
    "            \"6. DO NOT just import libraries without using them\"\n",
    "        )\n",
    "    }.get(task_type, \"Implement complete solution with meaningful operations\")\n",
    "    \n",
    "    structured_prompt = f\"\"\"\n",
    "Generate complete, self-contained Python code to solve:\n",
    "{prompt_text}\n",
    "\n",
    "Specific Instructions:\n",
    "{task_instructions}\n",
    "\n",
    "Code must:\n",
    "- Use ONLY numpy, pandas, sklearn, matplotlib\n",
    "- Print results clearly\n",
    "- For visualizations: plt.savefig('output.png')\n",
    "- Have NO unused imports\n",
    "- Be syntactically correct\n",
    "\n",
    "Code:\n",
    "```python\n",
    "\"\"\"\n",
    "    try:\n",
    "        output = generator(\n",
    "            structured_prompt,\n",
    "            temperature=0.1,\n",
    "            max_new_tokens=512,\n",
    "            truncation=True,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        return output[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def validate_code(generated_code):\n",
    "    if not generated_code:\n",
    "        return \"\", {\"numpy\": False, \"pandas\": False, \"sklearn\": False, \"matplotlib\": False}\n",
    "    if \"```python\" in generated_code:\n",
    "        code = generated_code.split(\"```python\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in generated_code:\n",
    "        code = generated_code.split(\"```\")[1].split(\"```\")[0]\n",
    "    else:\n",
    "        code = generated_code\n",
    "    lib_usage = {\n",
    "        \"numpy\": False,\n",
    "        \"pandas\": False,\n",
    "        \"sklearn\": False,\n",
    "        \"matplotlib\": False\n",
    "    }\n",
    "    repairs = [\n",
    "        (r\"from\\s+sklearn\\s+import\\s+\\*\", \n",
    "         \"from sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.cluster import KMeans\"),\n",
    "        (r\"classifier\\.fit\\(\\)\", \"classifier.fit(X_train, y_train)\"),\n",
    "        (r\"predict\\(\\)\", \"predict(X_test)\"),\n",
    "        (r\"plt\\.show\\(\\)\", \"plt.savefig('output.png')\"),\n",
    "        (r\"import matplotlib\\.pyplot as plt\", \n",
    "         \"import matplotlib.pyplot as plt\\nplt.switch_backend('Agg')\"),\n",
    "        (r\"\\.to_csv\\('data\\.csv'\\)\", \"\")\n",
    "    ]\n",
    "    for pattern, replacement in repairs:\n",
    "        code = re.sub(pattern, replacement, code)\n",
    "    if \"np.\" in code or \"numpy.\" in code:\n",
    "        lib_usage[\"numpy\"] = True\n",
    "    if \"pd.\" in code or \"pandas.\" in code:\n",
    "        lib_usage[\"pandas\"] = True\n",
    "    if \"train_test_split\" in code or \"RandomForestClassifier\" in code or \"KMeans\" in code:\n",
    "        lib_usage[\"sklearn\"] = True\n",
    "    if \"plt.\" in code or \"matplotlib.\" in code:\n",
    "        lib_usage[\"matplotlib\"] = True\n",
    "    required_imports = []\n",
    "    if lib_usage[\"numpy\"]:\n",
    "        required_imports.append(\"import numpy as np\")\n",
    "    if lib_usage[\"pandas\"]:\n",
    "        required_imports.append(\"import pandas as pd\")\n",
    "    if lib_usage[\"matplotlib\"]:\n",
    "        required_imports.append(\"import matplotlib.pyplot as plt\")\n",
    "    if lib_usage[\"sklearn\"]:\n",
    "        required_imports.append(\"from sklearn.ensemble import RandomForestClassifier\")\n",
    "        required_imports.append(\"from sklearn.model_selection import train_test_split\")\n",
    "        required_imports.append(\"from sklearn.metrics import accuracy_score\")\n",
    "    for imp in required_imports:\n",
    "        if imp not in code:\n",
    "            code = imp + \"\\n\" + code\n",
    "    code_lines = code.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in code_lines:\n",
    "        if line.strip().startswith(\"import\") or line.strip().startswith(\"from\"):\n",
    "            lib_name = line.split()[1].split(\".\")[0] if \"import\" in line else line.split()[1]\n",
    "            if lib_name in [\"numpy\", \"np\", \"pandas\", \"pd\", \"sklearn\", \"matplotlib\", \"plt\"]:\n",
    "                if any(f\"{lib_name}.\" in line for line in code_lines):\n",
    "                    cleaned_lines.append(line)\n",
    "        else:\n",
    "            cleaned_lines.append(line)\n",
    "    return '\\n'.join(cleaned_lines), lib_usage\n",
    "\n",
    "def safe_execute(code: str, data=None):\n",
    "    if not code:\n",
    "        return {\"status\": \"error\", \"message\": \"Empty code\"}\n",
    "    safe_env = {\n",
    "        \"__builtins__\": {\n",
    "            'print': print, 'range': range, 'len': len, 'str': str, 'int': int, \n",
    "            'float': float, 'bool': bool, 'list': list, 'dict': dict, 'tuple': tuple, \n",
    "            'set': set, 'min': min, 'max': max, 'sum': sum, 'abs': abs, 'round': round,\n",
    "            'enumerate': enumerate, 'zip': zip\n",
    "        },\n",
    "        \"np\": np,\n",
    "        \"pd\": pd,\n",
    "        \"plt\": plt,\n",
    "        \"RandomForestClassifier\": RandomForestClassifier,\n",
    "        \"KMeans\": KMeans,\n",
    "        \"train_test_split\": train_test_split,\n",
    "        \"accuracy_score\": accuracy_score,\n",
    "    }\n",
    "    if data is not None:\n",
    "        safe_env[\"df\"] = data\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        exec(code, safe_env)\n",
    "        return {\"status\": \"success\", \"env\": safe_env}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"{type(e).__name__}: {str(e)}\"}\n",
    "\n",
    "def run_demo():\n",
    "    test_prompts = load_test_datasets()\n",
    "    results = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not CPU_MODE else \"cpu\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"./final-model\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    generator = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Testing Base Model on {len(test_prompts)} Prompts\")\n",
    "    print(\"=\"*60)\n",
    "    for i, item in enumerate(test_prompts):\n",
    "        print(f\"\\n\\n{'='*40}\")\n",
    "        print(f\"PROMPT {i+1}/{len(test_prompts)} [{item['source']}]\")\n",
    "        print(f\"Type: {item.get('type', 'N/A')}\")\n",
    "        print(f\"Content:\\n{item['text'][:500]}...\")\n",
    "        print('-'*40)\n",
    "        generated = generate_robust_code(generator, item[\"text\"], item.get(\"type\", \"\"))\n",
    "        code, lib_usage = validate_code(generated)\n",
    "        print(\"\\nGENERATED CODE:\")\n",
    "        print('-'*40)\n",
    "        print(code)\n",
    "        print('-'*40)\n",
    "        data = item.get(\"data\", None)\n",
    "        exec_result = safe_execute(code, data)\n",
    "        result = {\n",
    "            \"prompt_id\": i+1,\n",
    "            \"source\": item[\"source\"],\n",
    "            \"type\": item.get(\"type\", \"\"),\n",
    "            \"generated_code\": code,\n",
    "            \"execution_status\": exec_result[\"status\"],\n",
    "            \"lib_usage\": lib_usage\n",
    "        }\n",
    "        if exec_result[\"status\"] == \"success\":\n",
    "            print(\"\\nEXECUTION SUCCESS!\")\n",
    "            print(\"Output captured in environment\")\n",
    "            result[\"output\"] = \"Execution completed successfully\"\n",
    "        else:\n",
    "            print(f\"\\nEXECUTION ERROR: {exec_result['message']}\")\n",
    "            result[\"error\"] = exec_result[\"message\"]\n",
    "        results.append(result)\n",
    "        print(f\"\\nCompleted prompt {i+1}/{len(test_prompts)}\")\n",
    "    results_filename = f\"base_model_results_{timestamp}.json\"\n",
    "    results_path = os.path.join(results_dir, results_filename)\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Test completed! Results saved to {results_path}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nEXECUTION SUMMARY:\")\n",
    "    print('-'*60)\n",
    "    success_count = sum(1 for r in results if r[\"execution_status\"] == \"success\")\n",
    "    print(f\"Successful executions: {success_count}/{len(results)}\")\n",
    "    for i, r in enumerate(results):\n",
    "        status = \"SUCCESS\" if r[\"execution_status\"] == \"success\" else f\"ERROR: {r.get('error', 'Unknown')}\"\n",
    "        print(f\"Prompt {i+1}: {r['source']} ({r['type']}) -> {status}\")\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    results = run_demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
