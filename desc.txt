Title :
SLMs for specific objectives on resource constrained devices	

Description:
We will work to develop a small language model for a specific objective, e.g. reading machine logs, that can be trained on and do inference on a resource constrained device.

Train and Test Datasets:
https://huggingface.co/datasets/millawell/wikipedia_field_of_science
https://huggingface.co/datasets/David-Xu/astronomy-stack-dpo-text
https://github.com/allenai/scidocs
https://huggingface.co/datasets/bigcode/the-stack

Models Used:
bigcode/tiny_starcoder_py
TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T

Optimization Techinques:
Pruning (both tinyllama and starcoder)
Weight Sharing (only starcoder)
Early Exit (only starcoder)

Results:

TinyLLama:
================================================================================
Optimization Results
================================================================================
 Technique Latency (ms) Throughput (samples/s) Memory (MB) Inference Success (%) Valid Code (%) Execution Success (%) Quality Score
Base Model     10057.68                   0.10      4208.8                 100.0           43.6                  56.9          65.6
   Pruning      5402.01                   0.19      4209.3                 100.0           39.6                  47.5          62.0

StarCoder:
================================================================================
Optimization Results
================================================================================
     Technique Latency (ms) Throughput (samples/s) Memory (MB) Inference Success (%)  Syntax Errors Execution Success (%) Quality Score
    Base Model       2664.1                   0.34       731.5                 100.0              5                   0.0           0.0
       Pruning       2737.2                   0.33      1389.7                 100.0              0                   0.0           0.0
Weight Sharing       2711.6                   0.34      1389.9                 100.0              1                   0.0           0.0
    Early Exit       2571.3                   0.36      1390.2                 100.0              3                   0.0           0.0

Training :
import json
import os
import subprocess
from datasets import Dataset, load_dataset
from typing import List

def prepare_datasets() -> Dataset:
    """Prepare dataset using only real data sources"""
    all_prompts: List[str] = []
    all_codes: List[str] = []
    
    # 1. SciDocs Dataset (25,000 target)
    def load_scidocs() -> List[str]:
        try:
            os.makedirs("scidocs_data", exist_ok=True)
            if not os.path.exists("scidocs_data/paper_metadata_view_cite_read.json"):
                subprocess.run([
                    "aws", "s3", "sync", "--no-sign-request",
                    "s3://ai2-s2-research-public/specter/scidocs/",
                    "scidocs_data/", "--region", "us-west-2", "--quiet"
                ], check=True)
            
            with open("scidocs_data/paper_metadata_view_cite_read.json", "r") as f:
                data = json.load(f)
            
            prompts = []
            for paper_id, content in data.items():
                title = content.get('title', '') or ''
                abstract = content.get('abstract', '') or ''
                
                if len(title) > 10 and len(abstract) > 200:
                    prompts.append(
                        f"Generate Python code for: {title}\nAbstract: {abstract[:400]}"
                    )
            return prompts
        except Exception as e:
            print(f"SciDocs loading failed: {str(e)}")
            return []

    # 2. Astronomy Dataset (15,000 target)
    def load_astronomy() -> List[str]:
        try:
            ds = load_dataset("David-Xu/astronomy-stack-dpo-text", split="train")
            return [example['prompt'] for example in ds]
        except Exception as e:
            print(f"Astronomy dataset loading failed: {str(e)}")
            return []

    # 3. Science Classification (15,000 target)
    def load_science() -> List[str]:
        try:
            ds = load_dataset("millawell/wikipedia_field_of_science", split="train")
            return [text for text in ds['text'] if len(text) > 30]
        except Exception as e:
            print(f"Science dataset loading failed: {str(e)}")
            return []

    # 4. Code Samples (20,000 target)
    def load_code_samples() -> List[str]:
        try:
            ds = load_dataset("bigcode/the-stack", 
                            data_dir="data/python", 
                            split="train",
                            streaming=True)
            
            samples = []
            for sample in ds:
                content = sample["content"]
                if any(imp in content for imp in ["numpy", "sklearn", "pandas", "matplotlib"]):
                    if "auto-generated" not in content.lower():
                        samples.append(content[:2000])
                        if len(samples) >= 20000:
                            break
            return samples
        except Exception as e:
            print(f"Code dataset loading failed: {str(e)}")
            return []

    # Load all datasets
    scidocs = load_scidocs()[:25000]  # Cap at 25k
    astronomy = load_astronomy()[:15000]  # Cap at 15k
    science = load_science()[:15000]  # Cap at 15k
    code_samples = load_code_samples()[:20000]  # Cap at 20k
    
    # Science code prompts (10k from science dataset)
    science_code_prompts = [
        f"Generate Python code for: {text.split(':')[-1].strip()}" 
        for text in science[:10000]
    ]

    # Combine all sources
    all_prompts.extend(scidocs)
    all_prompts.extend(astronomy)
    all_prompts.extend(science)
    all_prompts.extend(science_code_prompts)
    all_prompts.extend(["Generate Python code:"] * len(code_samples))
    
    all_codes.extend([""] * (len(scidocs) + len(astronomy) + len(science) + len(science_code_prompts)))
    all_codes.extend(code_samples)

    # Final dataset
    return Dataset.from_dict({
        "prompt": all_prompts,
        "code": all_codes
    })

# Create and verify dataset
dataset = prepare_datasets()
print(f"Final dataset size: {len(dataset)}")
print("Sample prompts:", dataset["prompt"][:3])
print("Sample codes:", [c[:100] + "..." if c else "" for c in dataset["code"][-3:]])